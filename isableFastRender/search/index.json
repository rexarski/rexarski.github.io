[{"content":"How to pick more beautiful colors for your data visualizations:\n You have lots of options. Which means you can stay in a small area of the color wheel and still have many options. Which means:\nDonâ€™t dance all over the color wheel.\n This is a perfect starting tutorial for people, who knows some superficial knowledge about design and eagerly want to improve his/her aesthetic sense, like me.ğŸ™Œ\n","date":"2020-09-08","permalink":"//localhost:1313/post/2020-09-08-beautifulcolors/","tags":["Visualization","Design"],"title":"Color Selection in Data Visualization"},{"content":"Edit: According to Miniflux\u0026rsquo;s documentation, refreshing feeds is not possible with Reeder since no user information is sent. This means, I can only fetch something new when the server side refreshes (by default, it\u0026rsquo;s once an hour), no matter what syncing frequency I set in the application. That\u0026rsquo;s definitely a bummer. But I will keep an eye on this. Maybe it will eventually become a turning point when I finally quit information overdosing.\n Oceans rise, empires fall. RSS will be back.\n My subscription to Inoreader will expire soon. It\u0026rsquo;s been a good year with it, but it was in fact less useful than I expected. From a retrospective point of view, the possibility to subscribe to more than 150 feeds might be the only feature that still seems attractive to me. But hey, why not more than 250, or 500? Although at the same time, I truly believe in the philosophy of \u0026ldquo;less is more\u0026rdquo; and \u0026ldquo;I probably won\u0026rsquo;t have enough time for that\u0026rdquo;.\nSo, three steps to unleash the true power of a personal RSS subscription service.\n Set up a Ubuntu server. (Optional) Deploy RSSHub on that server. Deploy Miniflux on that server.  Set Up A New Ubuntu Server I\u0026rsquo;m going to skip some introduction and simply paste the script here. Just follow Docker\u0026rsquo;s documentation to install docker and docker-compose on Ubuntu.\napt-get update apt-get dist-upgrade -y update-alternatives --config editor # set up docker repository sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common # Add Dockerâ€™s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; # install docker engine sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io sudo docker run hello-world # install docker-compose sudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # if failed, create a symbolic link # sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose # verified docker-compose --version  Deploy RSSHub RSSHub is an open source RSS feed aggregator.\n# docker compose deployment wget https://raw.githubusercontent.com/DIYgod/RSSHub/master/docker-compose.yml docker volume create redis-data docker-compose up -d # update by removing old containers # docker-compose down # docker deployment docker pull diygod/rsshub docker run -d --name rsshub -p 1200:1200 diygod/rsshub # stop container # docker stop rsshub # docker rm rsshub # then repeat installation  Deploy Miniflux After reading a post from Luke Singham, I\u0026rsquo;ve decided pick Miniflux as my self-hosted RSS service. It\u0026rsquo;s simple, looks clean, and easy to deploy.\nmkdir miniflux cd miniflux vim docker-compose.yml  Remember to keep a record of both pairs of usernames and passwords.\nversion: \u0026quot;3\u0026quot; services: miniflux: image: miniflux/miniflux:latest ports: - \u0026quot;80:8080\u0026quot; depends_on: - db environment: - DATABASE_URL=postgres://db_username:db_password@db/miniflux?sslmode=disable - RUN_MIGRATIONS=1 - CREATE_ADMIN=1 - ADMIN_USERNAME=miniflux_username - ADMIN_PASSWORD=miniflux_password db: image: postgres:latest environment: - POSTGRES_USER=db_username - POSTGRES_PASSWORD=db_password volumes: - miniflux-db:/var/lib/postgresql/data volumes: miniflux-db:  docker-compose up -d db docker-compose up -d miniflux  So now I can access the Miniflux service by going to my ip address. Input the Miniflux username and password, then we are all good. In addition, I need to enable Fever api in Settings in order to read my feeds in other third party applications like Reeder.\nNote: the \u0026ldquo;server\u0026rdquo; blank to fill in should be my ip/url + /fever.\n","date":"2020-08-29","permalink":"//localhost:1313/post/2020-08-29-self-host-rss/","tags":["RSS","Self-host"],"title":"Self-host RSS"},{"content":"This post is a subsequent blog to my daily experience basically in the last two months. As the general situation of the COVID-19 pandemic in Australian Capital Territory keeps is keeping stable, sometimes I almost forget to check on this Shortcut.\nLong story short, general idea is if you have very specific information in need, you can always create a temporary iOS Shortcut to assist you in capturing such information. In my case, it is the number of confirmed cases in ACT. Very specific. Therefore, all I need to do is to go to the governmentâ€™s website, find the page containing that value, and scrape it in HTML. And BOOM, you get it out of there.\nIâ€™ll specify the steps in the context of Shortcut:\n Create an action called â€œURLâ€. Insert the value as https://www.health.act.gov.au/about-our-health-system/novel-coronavirus-covid-19. Create an action called â€œGet Contents of URLâ€ with method GET. Create an action called â€œMake rich text from HTMLâ€. Replace HTML with Contents of URL. Create an action called â€œMatch Textâ€. Replace the two placeholders with (Confirmed cases \\d+) and Rich Text from HTML respectively. Create one last action called â€œShow resultsâ€. And change the last variable to Matches.   iCloud link to this Shortcut.\n","date":"2020-06-14","permalink":"//localhost:1313/post/2020-06-14-covid-shortcut/","tags":["iOS","Shortcut"],"title":"COVID confirmed cases Shortcut"},{"content":"99 days ago, at some moment on that day, I told myself â€œhey bro, itâ€™s time to kiss goodbye to those old residents crawling in your read-it-later-but-never-know-when-ish reading list and something like thatâ€.\nSo with that kept in mind, I have to jot down the number of items left in each categories like obsessive-compulsory disorder in Notion. Shout-out to Notion, it\u0026rsquo;s an amazing workspace for most of daily work. Back to the business, but I do really guess itâ€™s okay, 99 days have passed just in a flash. So I spent another 10 minutes this afternoon to replay this process in R.\nThe reproducible R code is attached in Gist.\n As you can see, in this 99 days of period, I actually cleaned up some to-dos like magic. I mean they are gone for good, hopefully I will not need to deal with them again. Therefore no chance of procrastination for them. On the other hand, I still have some increasing trend of hoarding to-watch videos on YouTube, which I have to point out that I only keep them for test preparation next month so that might not be as representative as it seems.\nAnywho, the best part of this decluttering project (Iâ€™d like to call that, sounds much cooler) is not only the Iâ€™ve cleaned up of my digital life (mostly), but I believe Iâ€™ve achieved some state of tranquility as well. That is to say, no matter how many unread articles in my RSS reader, no matter how many new episodes of my subscribed podcasts were released in the last 24 hours, and apparently no matter how many marvelous contents are there just existing on the Internet, I donâ€™t need to care about them at all.\nI ainâ€™t superman and I donâ€™t have unlimited concentration every day, I just donâ€™t need to give a damn about what is really going on in every details. Iâ€™d happily click a button to â€œread allâ€, â€œarchive allâ€ or â€œdelete allâ€.\nI donâ€™t wanna be overwhelmed by probably-not-that-useful miscellaneous stuffs.\nAnd happy 27th birthday to me.\n","date":"2020-06-13","permalink":"//localhost:1313/post/2020-06-13-decluttering/","tags":["Productivity","R"],"title":"99 days of Decluttering"},{"content":"Devon æœ¬æ„æ˜¯è‹±å›½çš„å¾·æ–‡éƒ¡ï¼Œç”±äºæ­¤åœ°çš„åœ°å±‚æœ€æ—©è¢«ç ”ç©¶ï¼Œä¾¿ç”¨ Devonian ç§°å‘¼åœ¨æ­¤å¤„å‘ç°çš„ç‰¹åœ°åœ°å±‚ã€‚å°”åç¿»è¯‘ä¸ºä¸­æ–‡ä¾¿å”¤åšã€Œæ³¥ç›†çºªã€ã€‚\né‚£ä¹ˆ DEVONthink 3 æ˜¯æˆ‘åœ¨ macOS ä¸Šå¸¸ç”¨çš„æ–‡æ¡£ç®¡ç†å·¥å…·ï¼ˆè™½ç„¶å®ƒèƒ½å¤Ÿå‘æŒ¥çš„ä½œç”¨è¿œå¤§äºæ­¤ï¼‰ï¼Œä½†è‡ªå·±å…¶å®å¹¶æ²¡æœ‰å¼„æ¸…æ¥šå®ƒçš„å¤‡ä»½ä¸åŒæ­¥åŠŸèƒ½ï¼Œå°¤å…¶æ˜¯å’Œå®ƒè‡ªå·±çš„ iOS ç‰ˆæœ¬ app ä¹‹é—´çš„å…³ç³»æ›´æ˜¯ä¸€å¤´é›¾æ°´ã€‚å¶ç„¶ä¹‹é—´ä»Šå¤©å‘ç°æœ¬åœ° iCloud Drive æ–‡ä»¶å¤¹è™½ç„¶åªæœ‰å¤§çº¦å°å‡ ç™¾ mb çš„æ–‡ä»¶ï¼Œä½†ä» ~/Library/Mobile Documents è·¯å¾„è®¿é—®åˆ™ä¼šçœ‹åˆ°ä»¤äººå‘æŒ‡çš„ 30gb æ–‡ä»¶å¤§å°ã€‚è€Œä¸”éå¸¸ç¡®å®šçš„æ˜¯ï¼Œè¿™äº›æ–‡ä»¶æ—¢å ç”¨äº†æœ¬åœ°ç¡¬ç›˜ï¼Œåˆå ç”¨äº† iCloud Drive å­˜å‚¨ç©ºé—´ï¼Œè€Œä¸”è¿˜å¹¶æ²¡æœ‰åº”æœ‰çš„ä½œç”¨ã€‚äºæ˜¯ä¹ï¼Œæœ¬ç€æ•‘ç¡¬ç›˜äºæ°´ç«çš„æƒ³æ³•ï¼Œæƒ³è¦æŠŠå†—ä½™çš„å¤‡ä»½æ¸…æ‰«å¹²å‡€ã€‚\né¦–å…ˆè¦åšçš„äº‹æƒ…æ˜¯ï¼Œåœ¨ macOS ç‰ˆæœ¬çš„ DEVONthink ä¸­å°†åŸæœ‰çš„ sync location åå‘å‹¾é€‰ iCloud è¿™ä¸€é¡¹ã€‚ï¼ˆæœ¬åœ°å¤‡ä»½è¿˜æ˜¯æ¯”è¾ƒæ˜æœ—çš„ï¼Œæ’ä¸€å—å¤–æ¥ç¡¬ç›˜ï¼Œé€‰æ‹©æƒ³è¦å¤‡ä»½çš„ databases å°±ç®—å®Œå·¥äº†ã€‚è€Œä¸”å¤‡ä»½æ–‡ä»¶å¯ä»¥ç›´æ¥é€šè¿‡ Finder è®¿é—®ï¼Œè¦æ€è¦å‰ä»»å›ä½¿å”¤ã€‚ï¼‰\nç„¶åè¦åšçš„å°±æ˜¯å» iCloud ä¸­æ‰¾åˆ° DEVONthink å¤‡ä»½æ‰€å ç”¨çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚æ­¤å¤„æœ‰ä¸¤ç§æ–¹æ³•ï¼š\n ç¬¬ä¸€ç§æ–¹æ³•ä»…é™ç”¨äºç”¨æˆ·å†å²ä»¥æ¥åªé€šè¿‡ iCloud å¤‡ä»½è¿‡ä¸€æ¬¡æ•°æ®çš„æƒ…å†µä¸‹ã€‚é€šè¿‡ iOS çš„ç³»ç»Ÿè®¾ç½® Settings - Apple ID - iCloud - Manage Storage - DEVONthink To Go - Delete Data å°±å¯ä»¥å®Œæˆã€‚ä½†å®é™…æƒ…å†µå¾€å¾€ä¸æ˜¯è¿™æ ·ï¼Œåœ¨å…ˆå‰çš„æ— æ•°æ¬¡å°è¯•ä¸­ï¼Œæˆ–å¤šæˆ–å°‘æˆ‘å·²ç»å°è¯•è®¾ç½®è¿‡å¤šæ¬¡ä»¥ iCloud ä¸ºå¤‡ä»½åœ°å€ï¼Œé‚£ä¹ˆå®é™…ä¸Š iCloud ä¸­å°±ä¼šåœ¨ä¸€ä¸ªè·¯å¾„ä¸‹å­˜æ”¾æœ‰å¤šæ¬¡å¤‡ä»½æ–‡ä»¶ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘åªæ˜¯å¤‡ä»½äº† Inbox å’Œå¦å¤–ä¸€ä¸ªå° databaseï¼Œä½†æ€»çš„å¤‡ä»½æ–‡ä»¶æ‰€å ç©ºé—´æœ‰ 30gbã€‚å…·ä½“è€Œè¨€ï¼Œå°±æ˜¯å…ˆå‰æˆ‘ä¸€å®šå¤‡ä»½è¿‡æ›´å¤šçš„æ•°æ®ï¼Œè€Œå¤‡ä»½çš„æ®‹éª¸éƒ½ç•™å­˜åœ¨äº† iCloud çš„è¿™ä¸ªå¤‡ä»½è·¯å¾„ä¸‹ã€‚ ç¬¬äºŒç§æ–¹æ³•ï¼Œå°±æ˜¯ç›´æ¥æ€åˆ° iCloud é‡Œå¹²æ‰æ•´ä¸ªå¤‡ä»½æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæƒ³ç›´æ¥é€šè¿‡ Finder è®¿é—®ï¼Œä½ å¤šåŠåªæ˜¯çœ‹åˆ°äº† iCloud Drive é‡Œé›¶é›¶æ˜Ÿæ˜Ÿçš„ä¸€äº›æ–‡ä»¶ã€‚è¦çŸ¥é“å•Šï¼ŒiCloud Drive åªæ˜¯ iCloud çš„ä¸€ä¸ªå­é›†ã€‚æ‰€ä»¥æ ¸å¿ƒé—®é¢˜å°±åœ¨äºï¼Œæˆ‘æƒ³å»çœ‹å­˜åœ¨ iCloud é‡Œçš„ DEVONthink å¤‡ä»½è·¯å¾„ï¼Œä¸ºä»€ä¹ˆä½ éæŠŠæˆ‘å¾€æ”¾æ–‡ä»¶çš„è·¯å¾„å¸¦å‘¢ï¼Ÿç»ˆç«¯èµ°èµ·ã€‚  cd ~/Library/Mobile\\ Documents ls rm -rf iCloud~679S2QUWR8~com~devon-technologies~sync  âš ï¸ æ³¨æ„ï¼š åœ¨ rm -rf ä¹‹å‰è¿˜æ˜¯ ls ç¡®è®¤ä¸€ä¸‹è¿™ä¸ªå¤‡ä»½è·¯å¾„ç¡®å®å­˜åœ¨ï¼ŒåŒ…å« com-devon-technologies çš„å…³é”®è¯ï¼Œç„¶åç”¨ sudo æƒé™æ¸…ç†å¹²å‡€ã€‚\næ­¤æ—¶ï¼Œæˆ‘ä¸ç¦è”æƒ³åˆ°ï¼Œé‚£æˆ‘æ›¾ç»å°è¯•è¿‡çš„åˆ«çš„ sync locations ä¼šä¸ä¼šä¹Ÿæœ‰åŒæ ·çš„ã€Œéšè—å¤‡ä»½æ–‡ä»¶ã€çš„é—®é¢˜å‘¢ï¼Ÿäºæ˜¯æˆ‘å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé‡æ–°æ·»åŠ äº† Dropbox ä½œä¸ºå¤‡ä»½åœ°å€ã€‚å¤‡ä»½æ•°æ®åº“æˆ‘é€‰æ‹©äº†åªæœ‰ä¸‰ä¸ªæ–‡ä»¶çš„ Global Inboxï¼Œæ–¹ä¾¿è¯•é”™ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æ–°å»ºå¤‡ä»½åç§°çš„æ—¶å€™ DEVONthink å±…ç„¶ç»™æˆ‘è‡ªåŠ¨è”æƒ³äº†ä¸¤ä¸ªæˆ‘æ›¾ç»çš„å¤‡ä»½åç§°ï¼æˆ‘æœ¬ä»¥ä¸ºå½“æ—¶åœ¨ DEVONthink çš„è®¾ç½®é‡Œåˆ é™¤æ‰è¿™ä¸ªå¤‡ä»½è·¯å¾„ï¼Œå°±è‡ªåŠ¨æŠŠå¤‡ä»½è·¯å¾„é‡Œçš„å¤‡ä»½æ–‡ä»¶ä¹Ÿåˆ é™¤äº†ï¼Œç»“æœå¹¶ä¸æ˜¯è¿™æ ·ã€‚æˆ‘è¯•å›¾ä» Dropbox çš„æ¡Œé¢å®¢æˆ·ç«¯è®¿é—®åˆ° Apps æ–‡ä»¶å¤¹ï¼Œçœ‹ä¸€çœ‹ DEVONthink æ˜¯å¦çœŸçš„æœ‰å‡ ä¸ªå¤‡ä»½æ–‡ä»¶å­˜åœ¨è¿™é‡Œï¼Œç»“æœå‘ç°è¿™ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºä¸ Dropbox å…³è”çš„ Linked Appï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨æœ¬åœ°ã€‚åˆ‡æ¢åˆ°ç½‘é¡µç«¯ï¼Œå´çœŸçš„å‘ç°äº†æ›¾ç»çš„ä¸¤ä¸ªå¤‡ä»½æ–‡ä»¶éƒ½é™é™åœ°èººåœ¨ Dropbox çš„å­˜å‚¨ç©ºé—´é‡Œã€‚æœæ–­é€‰æ‹©äº†åˆ é™¤ï¼Œäºæ˜¯ Dropbox çš„ç©ºé—´ä¹Ÿå¾—åˆ°äº†é‡Šæ”¾ã€‚\næ‰€ä»¥ï¼Œæ€»ç»“ä¸€ä¸‹ä»Šå¤©çš„å‘ç°ï¼š\nåœ¨ DEVONthink é‡Œåˆ é™¤ sync locations å¹¶ä¸ç­‰åŒäºåˆ é™¤å·²ç»åˆ›å»ºçš„å¤‡ä»½æ–‡ä»¶ã€‚å¦‚æœéœ€è¦åˆ é™¤å¤‡ä»½æ–‡ä»¶ï¼Œéœ€è¦åˆ°æŒ‡å®šå¤‡ä»½åœ°å€çš„ explicit location å»å¼ºè¡Œåˆ é™¤è¿™äº›å¤‡ä»½æ–‡ä»¶ï¼›å¦åˆ™åˆ™ä¼šæœ‰å¾ˆå¤šç©ºé—´è¢«å·å·å ç”¨ï¼Œæ ¹æœ¬ä¹Ÿæ— ä»çŸ¥æ™“ã€‚\n","date":"2020-06-09","permalink":"//localhost:1313/post/2020-06-09-devonthink-backup-cleanup/","tags":["macOS","Productivity"],"title":"DEVONthink å¤‡ä»½æ¸…æ‰«"},{"content":"As a fellow redditor, I can naturally tell you what is trending on the /r/dataisbeautiful subreddit. Beautiful (or maybe not) charts come and go. One or two years back, there were ridge plots aka joy plots. Then starting from March 2019, the bar chart races swiped the the subreddit and YouTube. I tried to take a dip since then but could not finish one before its moratorium.\nStill, it is a fun chart and its â€œracingâ€ style definitely outplays any regular bar charts. So even being late to the party, I would like to try it on my own. By saying â€œon my ownâ€, I actually mean â€œreplicating othersâ€™ workâ€ and make some changes to see whatâ€™s going on.\n And yes there are lots of problems in this racing bar chart, for example, the subtitle is so closed to title, the color palette could be more carefully picked etc. But Iâ€™m just gonna leave it here because this is the â€œsmall dipâ€ I was talking about. Not diving in too deep.\n","date":"2020-06-05","permalink":"//localhost:1313/post/2020-06-05-late-party/","tags":["R","Visualization"],"title":"Late to the party"},{"content":"Inspired by Stefanâ€™s blog post, I set up my own photo stream as well. Itâ€™s called Air Avo, a self-hosted photo scream based on a repo by Tim Van Damme.\nA bunch of funny things need to point out:\n Modify photo-stream/_includes/head.html to change the favicon. Modify photo-stream/index.html to change the link in the right bottom corner of the stream page. I airdropped some photos from my iPhone to my MacBook. Some of them share an extension of .JPG instead of .jpg. Such photos wonâ€™t be rendered in the build. I manually change them back to .jpg but Git thinks they are literally the same. The iOS Shortcut provided by Stefan requires audio dictation in order to name the photo. Considering the fact that Siri has a funky support of Mandarin recognition, I create a new version with text input as file name. You can download the Shortcut here.  Overall, itâ€™s an awesome minimalist website and perfectly fits my â€œanti-instagramâ€ mindset. Very easy to deploy, strongly recommended to give it a try and maybe make some tweaks to it.\n","date":"2020-04-21","permalink":"//localhost:1313/post/2020-04-21-air-avo/","tags":["Food"],"title":"Air Avo is online!"},{"content":"tl;dr Implemented an alerting (kinda hard to call it \u0026ldquo;monitoring\u0026rdquo;) system for Django with R Server and a Telegram bot.\nA typical instance of its report is like this:\n Some errors occurred in the last 15 minutes on the server, a copy of log will be processed by sentRy. It will identify those new errors which have not been reported yet and save them to local storage. Meanwhile, the incremental part will be parsed to a telegram bot, sending error summaries and a recent 12-hour bar chart to a channel. At the same time, an updated copy of notifications (of course, errors) will be synced to Shinyapps.io and the Shiny app should have the latest info displayed.\n It was designed to fulfill a particular job and I guess it got things done to some extent. But recently, our dev team deployed a fully functional monitoring system called Sentry. I mean, what a coincidence. I had no idea about this and only named it after the sentry gun in Team Fortress 2.\nDependencies  tidyverse telegram.bot cronR shinyFiles aws.s3  How to use it (anyway)?  In Telegram, create a new bot under the permission of @BotFather. Follow the order and make sure you have a valid API Token. You can test the basic message functionality with bot_script.r. But it won\u0026rsquo;t be needed in the main script. You can also test run the actual process with log_process.r. If there are problems no more, click Addin in RStudio and select \u0026ldquo;Schedule R scripts on Linux/Unix\u0026rdquo;.  Related files  bot_script.r The script for testing Telegram bot creation. Will be dropped later. dashboard A shiny app displaying all the errors. error.log An error log copied from Django. global.r The script dedicated to setting up S3 connection. log_process.r The main script which needs to run periodically. notification.csv The formatted backup of errors captured by this monitoring script. It is de facto very similar to error.log. settings.csv A file to save some setting parameters including the latest reported error time. log_process.log The R console log for running log_process.r. Potentially useful when debugging.  Issues   Even the files published to shinyapps.io do not involve any unchecked files (when publishing), they will be verified any ways, thus leading to some filename and path related error returning. A better idea is to create a separate folder and zip it before uploading. Since I claimed all the paths absolute in my code due to the limitation of cronR, I have to scp another copy to the directory of shiny after writing to notification.\n  You can always refer to the \u0026ldquo;Log\u0026rdquo; tab in shiny app to debug. Really helpful.\n  Very hard to read a csv file without any column names in S3 bucket. The function aws.s3::s3read_using(FUN, ..., object, bucket, opts = NULL) is problematic, as FUN cannot insert any extra parameters. It is a shame that readr::read_delim cannot be used as well. At last a blog post on Medium saved my life.\n  Something weird happens when meta1 and meta2 extracted have different length. Specifically, meta1 with â€œERROR|WARNING|CRITICALâ€ has fewer than the number of rows. That is to say, some lines are not starting with â€œERRORâ€¦â€ Turns out my regex should start with a ^ otherwise things like \u0026quot; self._log(ERROR, msg, args, **kwargs)â€ could be matched as well. In short,\n  ^ ","date":"2020-02-14","permalink":"//localhost:1313/post/2020-02-14-sentry/","tags":["AWS","Telegram","R"],"title":"From sentRy to Sentry"},{"content":"ä¸¤ä¸ªå¥½ä¸œè¥¿ï¼š\nradian. å·ç§°æ˜¯ã€ŒA 21 century R consoleã€.\nå…¶å® 21 ä¸–çºªä¹Ÿéƒ½è¿‡äº† 20 å¹´äº†ï¼Œä¸‹ä¸ªæœˆ R 4.0 ä¹Ÿå°†èµ¶ç€ 20 å‘¨å¹´çš„æ—¥å­å‘å¸ƒã€‚ä½†è¿™éƒ½ä¸æ˜¯é‡ç‚¹ã€‚é‡ç‚¹æ˜¯æ¯æ¬¡ä¸ºäº†è¿è¡Œä¸€ä¸¤è¡ŒæŒ‡ä»¤è€Œæ‰“å¼€ã€Œç¬¨é‡ã€çš„ IDE ä»æ¥éƒ½ä¸æ˜¯æˆ‘æ‰€å–œæ¬¢çš„æ“ä½œè¡Œä¸ºã€‚ä¹‹å‰ç”¨è¿‡ console é‡Œçš„ Rï¼Œä½†æ€»è§‰å¾—å·®ç‚¹æ„æ€ã€‚å¦‚æœè¯´ç”¨ä¸€å¼ å›¾æ¥æ¦‚æ‹¬ radian çš„ä¼˜ç‚¹ï¼Œé‚£å°±æ˜¯è¿™æ ·çš„ï¼š\nå¦å¤–ä¸€ä¸ªæ˜¯è€æœ‹å‹ janitor package äº†ï¼Œæ›¾ç»åœ¨çœ‹ R for Data Science çš„æ—¶å€™ç”¨è¿‡ä¸€é˜µå­ï¼Œä½†æ˜¯ package è¿™ç§ä¸œè¥¿ä½ çœ‹åˆ°äº†ä¸ç”¨å°±åºŸäº†ï¼Œæˆ–è€…ç”¨è¿‡ä¸€æ¬¡ä¹‹åéš”ä¸€æ®µæ—¶é—´ä¸ç”¨ï¼Œä¹Ÿç­‰äºåºŸäº†ã€‚äºæ˜¯æŠ½ç©ºåœ¨ radian é‡ŒæŠŠå‡ ä¸ªä¸»è¦ functions è¿‡äº†ä¸€ä¸‹ï¼Œè§‰å¾—è¿˜æ˜¯åœ¨å¤„ç†ã€Œæ¯”è¾ƒè§„æ•´çš„ã€ç½‘ä¸Šä¸‹è½½çš„æ•°æ®ã€çš„æ—¶å€™æ¯”è¾ƒæ–¹ä¾¿ã€‚æ€»ç»“äº†ä¸€ä¸‹åŠŸç”¨ï¼š\n clean_names(). è½¬ä¸ºå°å†™+ä¸‹åˆ’çº¿çš„å˜é‡åæ ¼å¼ï¼Œèƒ½å¤Ÿå¤„ç†ç‰¹æ®Šç¬¦å·ã€‚ tably(). è¾“å…¥ vector è¾“å‡ºé¢‘ç‡å’Œé¢‘æ•°ç»Ÿè®¡ table.  å¦å¤–å¦‚æœå«æœ‰ NA åˆ™ä¸ä¼šè®¡å…¥ç»Ÿè®¡ã€‚ å¯ä»¥ç”¨ remove_empty(x, which = c(\u0026quot;rows\u0026quot;, â€œcolsâ€)) å»é™¤æ•´è¡Œå’Œï¼ˆæˆ–ï¼‰æ•´åˆ—ä¸º NAçš„æ•°æ®ã€‚ tably() åè·Ÿéšå¤šä¸ª vectors åˆ™è¾“å‡º crosstabulation ç»“æœã€‚   get_dupes(x). è¾“å…¥ data frame å’Œ å˜é‡å xï¼Œè¾“å‡ºé‡å¤çš„æ•°æ®ï¼ˆè¡Œï¼‰ã€‚ excel_numeric_to_data(). å°†æŸäº›è½½å…¥ Excel æ–‡ä»¶æ—¶è¢«è½¬åŒ–çš„æ—¥æœŸè¿˜åŸä¸º Date ç±»å‹ã€‚ ","date":"2020-01-31","permalink":"//localhost:1313/post/2020-01-31-radian-and-janitor/","tags":["R","Cleaning"],"title":"radian and janitor"},{"content":"Two weeks ago I sent my 2017 model of MacBook Pro to our local Genius Bar to replace the keyboard and the battery. During this one week span of absence, I moved to my iPad Pro to finish the Australian universities\u0026rsquo; course information scraping.\nSetup Initially I tried to deploy an server version of RStudio on my Ubuntu 19.04 server. But constant errors occured during the installation. After downgrading from 19.04 to 18.04. the installation went on smoothly.\nIn addtion, I installed zsh and oh-my-zsh as life-savers. (Guide)\nInstall R and RStudio server.\nStart RStudio Server will not permit logins by system users (those with user ids lower than 100).\nSo we need to create a \u0026ldquo;normal\u0026rdquo; user.\n$ sudo adduser rexarski // password==xingzilovexuanzi $ cat /etc/passwd($ grep '^rexarski' /etc/passwd  Unable to install R packages on Linux  Update and upgrade sudo apt updates and sudo apt upgrade sudo apt autoremove curl and sudo apt install curl no effect  ------------------------- ANTICONF ERROR --------------------------- Configuration failed because libcurl was not found. Try installing: * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc) * rpm: libcurl-devel (Fedora, CentOS, RHEL) * csw: libcurl_dev (Solaris) If libcurl is already installed, check that 'pkg-config' is in your PATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config is unavailable you can set INCLUDE_DIR and LIB_DIR manually via: R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...' -------------------------------------------------------------------- ERROR: configuration failed for package â€˜curlâ€™   Do these on root (who knows\u0026hellip;) https://askubuntu.com/questions/1057455/some-r-packages-wont-install  $ sudo apt-get install libcurl4-dev $ sudo apt-get install r-base-dev $ sudo apt-get install libssl-dev $ sudo apt-get install libxml2-dev   Then use a normal account (non-root) to install R packages.  install.packages(â€œtidyverseâ€, â€œrvestâ€, â€œpacmanâ€) # verify library(tidyverse)  Tidy problem  Takes forever to install tidyverse though. Also, pacman is not available for 3.4.4 Some packages are easy to install, but some takes forever like dplyr tidyr etc. Even loading an installed package dplyr is problematic:  Error: package or namespace load failed for â€˜dplyrâ€™ in library.dynam(lib, package, package.lib): shared object â€˜dplyr.soâ€™ not found In addition: Warning message: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_tibble.grouped_dfâ€™, â€˜as_tibble.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€˜dim.tbl_cubeâ€™, â€˜distinct.data.f [... truncated]  Turns out, while installing readr i found there is not enough virtual memory. See this github issue: CRAN version of readr crashes the compiler on small-RAM Ubuntu 16.04 system. The Vultr server I was using is a $5 per month 1GB RAM little kicker, so this pretty much explains everything.\nThanks to this post. I call this â€œblack magicâ€. You can actually install a prebuilt binary so that you do not have to compile the package from the source.\n$ apt-get update $ apt-get install software-properties-common $ add-apt-repository -y â€œppa:marutter/rrutterâ€ $ add-apt-repository -y â€œppa:marutter/c2d4uâ€ $ apt-get update $ apt-get install r-cran-readr  Worked for me this time. But for packages like dplyr which were installed previously but could not be loaded on my end, I tried to remove.packages() them first, then installed the prebuilt version. The binary this time would not save the day because it is outdated and could not support the minimum requirement of rlang.\nBut anyways, this whole process at least suffices to power me through scraping a few bunch of universities courses already.\nI am officially missing my lil buddy MacBook.\n","date":"2019-11-25","permalink":"//localhost:1313/post/2019-11-25-r-on-ipad/","tags":["R","iOS","RStudio"],"title":"Writing R codes on an iPad Pro"},{"content":"å¦‚æœé¡ºåˆ©çš„è¯ï¼Œ30 åˆ†é’Ÿåº”è¯¥å¤Ÿã€‚å®é™…ä¸Šæˆ‘å¤§æ¦‚èŠ±äº† 2 ä¸ªå°æ—¶ï¼Œä¸»è¦å‘¨æ—‹äºå„å®¶é‚®ä»¶æœåŠ¡æä¾›å•†ä¹‹é—´ã€‚\n æœ€è¿‘å‘ç°å¾ˆå¤šäº§å“çš„å”®å/ç¤¾ç¾¤éƒ½åœ¨ç”¨ä¸€ä¸ªã€Œç¥ç§˜ã€çš„ bbs æ¡†æ¶ï¼Œä¸€ç›´ä¸çŸ¥é“å…¶å°Šå§“å¤§åã€‚æ¯”å¦‚Agenda, Airtable, è¿˜æœ‰æŸäº›ç¥ç§˜çš„å°è®ºå› ğŸ˜…. å¤šç•ªå¯»è®¿åœ¨ GitHub æ‰¾åˆ°äº†è¿™ä¸ªåå« Discourse çš„å¼€æºé¡¹ç›®â€”â€”GitHub - discourse/discourse: A platform for community discussion. Free, open, simple.\nå› ä¸ºé¡¹ç›®éœ€è¦ï¼Œæˆ‘æ‰“ç®—ç°åœ¨è‡ªå·±çš„ 1GB RAM å°æœåŠ¡å™¨ä¸Šå…ˆå°è¯•ä¸€ä¸‹ã€‚æ­£å¦‚ Discourse å›¢é˜Ÿåœ¨ä»–ä»¬çš„å¸®åŠ©æ–‡æ¡£ä¸­æ‰€è¨€ï¼Œæƒ³è¦æˆåŠŸéƒ¨ç½²ä¸€ä¸ªæœ€å°éœ€æ±‚çš„ç½‘ç«™éœ€è¦ä¸‰æ ·ä¸œè¥¿ï¼š\n ä¸€å°ã€Œåˆšåˆšå¥½å¤Ÿç”¨ã€çš„æœåŠ¡å™¨ã€‚ ä¸€ä¸ªã€Œä½ è§‰å¾— ğŸ‘Œ å°± ğŸ‘Œã€çš„ä¾¿å®œåŸŸåã€‚ ä¸€ä¸ªã€Œåœ¨å…è´¹å’Œæ”¶è´¹è¾¹ç¼˜å¾˜å¾Šã€çš„é‚®ä»¶å‘é€æœåŠ¡å•†ã€‚  æœåŠ¡å™¨æˆ‘ä½¿ç”¨çš„æ˜¯ Vultr çš„ 1GB RAM, 25GB SSD, æ”¶è´¹ä¸º $5/month. åŸŸååˆ™æ˜¯åœ¨ GoDaddy è´­ä¹°çš„ã€‚åæ§½ä¸€ä¸‹ GoDaddy çš„ DNS è®¾ç½®ï¼Œç›¸æ¯”äº AWS æ“ä½œç®€å•ä¸å°‘ï¼Œä½†å¼•å¯¼å…¶å®æœ‰ç‚¹æ¨¡ç³Šã€‚é‚®ä»¶å‘é€æœåŠ¡å•†æˆ‘æœ¬æƒ³å¯ç”¨å·²ç»ä½¿ç”¨äº†ä¸€æ®µæ—¶é—´çš„ Mandrillï¼Œä½†æ˜¯å‘ç°å®ƒåœ¨æˆ‘æŠŠ Discourse è®¾ç½®åœ¨ subdomain ä¸Šæ—¶æ€»æ˜¯è¡Œä¸é€šã€‚\nå®˜æ–¹çš„å¸®åŠ©æ–‡æ¡£åœ¨é‚®ä»¶çš„é—®é¢˜ä¸Šæ¨èäº†ä¸å°‘æœåŠ¡æä¾›å•†ï¼Œè€Œæ ¹æ®æˆ‘ä¸ªäººçš„æ‘¸ç´¢ï¼Œæœ€ç»ˆé€‰æ‹©çš„æ˜¯ SendGridã€‚Free tier åœ¨è°ƒè¯•é˜¶æ®µåŸºæœ¬å¤Ÿç”¨ï¼Œå½“ç„¶æ­£å¼ä¸Šçº¿è‚¯å®šæ˜¯ä¸è¡Œçš„ã€‚å…·ä½“çš„æµé‡é™åˆ¶)ä¸ºæ¯å¤© 100 å°å…è´¹ã€‚\næœåŠ¡å™¨çš„éƒ¨ç½²æŒ‰ç…§æ¨èèµ° Ubuntu LTS ç‰ˆæœ¬å°±å¯ä»¥äº† ï¼Œä¸ç”¨åšè¿‡å¤šçš„è°ƒè¯•ã€‚ä¹‹å ssh ç™»å½•ï¼Œå¹¶ git clone æ•´ä¸ª Discourse é¡¹ç›®åˆ° /var/discourse è·¯å¾„ä¸‹ã€‚\nç„¶åæ˜¯ä¸€ç³»åˆ—ç‚¹ç‚¹ç‚¹ç‚¹ä¹±æ“ä½œï¼š\n åœ¨ SendGrid ä¸­æ–°å»ºä¸€ä¸ª api key. åœ¨ SendGrid ä¸­ Settings \u0026ndash; Sender Authentication â€” Domain Authentication. éªŒè¯è‡ªå·±æ˜¯é‚®ä»¶å‘é€åŸŸåçš„çœŸå®æ‹¥æœ‰è€…ï¼Œè¿™ä¸€æ­¥å°±æŒ‰ç€æç¤ºèµ°ã€‚ ç„¶åè½¬åˆ° GoDaddy çš„ DNS Settings æ–°å¢ä¸‰æ¡ CNAME Record. è¿™é‡Œæœ‰ä¸€ä¸ªæš—å‘ï¼šSendGrid ä¼šæç¤ºä½ è¿™ä¸‰æ¡ records çš„ host æ˜¯ something.example.com ä½†æ˜¯ä½ ä¸èƒ½ç›´æ¥å¤åˆ¶ç²˜è´´åˆ° GoDaddy ä¸­ï¼Œè€Œåªèƒ½ç²˜è´´ something ä½œä¸º host. è€Œ record çš„ value åˆ™ç›´æ¥æ”¾å¿ƒç²˜è´´ã€‚ ç„¶ååœ¨ GoDaddy ä¸­åˆ›ä¸€ä¸ª A Record, host ä¸ºä½ æƒ³è®¾ç½®çš„ subdomain nameï¼Œæ¯”å¦‚ talk, discourse, forumï¼Œä»ç„¶ä¸è¦å¸¦ example.com. value åˆ™å¡«å…¥ä½ çš„æœåŠ¡å™¨ public ip åœ°å€ã€‚  å›åˆ°å·²ç» ssh è¿æ¥çš„æœåŠ¡å™¨ï¼š\ncd /var/discourse ./discourse-setup  è¿‡ç¨‹ä¸­ç¢°åˆ°ä¸€ä¸ªæŠ¥é”™ï¼Œè¿”å› No Public Key Error. å‚è€ƒ Stack Overflow çš„è§£ç­”ï¼Œå°‘ä»€ä¹ˆ KEYID å°±ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤æ·»åŠ å°±å¯ä»¥ äº†ï¼š\nsudo apt-key adv â€”keyserver keyserver.ubuntu.com â€”recv-keys KEYID  éƒ¨ç½²è¿‡ç¨‹ä¸­æœ‰å¤šæ¬¡éœ€è¦å¡«å†™çš„ä¿¡æ¯ï¼š\nHostname\t:\tsubdomain.example.com æ¨èä½¿ç”¨æ³¨å†Œæ³¨å†ŒåŸŸåçš„æŸä¸ª subdomain Email\t: me@anything.com éšä¾¿æŒ‘é€‰ä¸€ä¸ªè‡ªå·±åœ¨ç”¨çš„é‚®ç®±åœ°å€ä½œä¸º admin çš„æ³¨å†Œé‚®ç®± SMTP address\t:\tsmtp.sendgrid.net æ˜¯ email service provider çš„å…·ä½“è®¾ç½® SMTP port\t: 587 åŒä¸Š SMTP username\t:\tapikey\tåŒä¸Š SMTP password :\t*****\tç”Ÿæˆ api çš„æ—¶å€™ä¸€èˆ¬ä¼šç»™å‡º  å¤§çº¦ 3 åˆ° 5 åˆ†é’Ÿçš„å®‰è£…æ—¶é—´ï¼Œç„¶åå°±å¯ä»¥ç›´æ¥è®¿é—® subdomain.example.com æ¥æŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸã€‚ä¸€èˆ¬æ¥è¯´å®‰è£…çš„é—®é¢˜ä¸å¤§ï¼Œä¸‹ä¸€æ­¥éªŒè¯ admin é‚®ç®±æ‰æ¯”è¾ƒå®¹æ˜“å‡ºé”™ï¼Œå¦‚æœå¤Ÿå¹¸è¿ï¼Œä¹‹å‰æ²¡æœ‰è¸©åˆ°æš—å‘ä¸€è¹´è€Œå°±ï¼Œä½ çš„ me@anything.com é‚®ç®±é‡Œåˆ™ä¼šå¤šå‡ºä¸€å°é‚®ä»¶ï¼ˆæ³¨æ„æŸ¥ spam ğŸ“®ï¼‰ã€‚éªŒè¯å®Œæˆå°±ä¼šçœ‹åˆ°å¦‚å›¾æ‰€ç¤ºï¼š\nå¤§åŠŸå‘Šæˆã€‚\nå¦‚æœä¸­é—´ç¢°åˆ°äº†ä»€ä¹ˆé—®é¢˜ï¼Œç›´æ¥åœ¨æœåŠ¡å™¨ä¸Šï¼ˆ/var/discourse) é‡è·‘ä¸€é ./discourse-setup æŒ‡ä»¤å°±å¯ä»¥äº†ã€‚å›°éš¾ä¸»è¦è¿˜æ˜¯é›†ä¸­åœ¨ä¸‰é¡¹æœåŠ¡çš„é…ç½®ã€‚\nå¦å¤–æŒ‡å‡ºä¸€ç‚¹ï¼Œè™½ç„¶åœ¨æˆ‘å®‰è£…è¿‡ç¨‹ä¸­å¸®åŠ©ä¸å¤§ï¼Œä½† Discourse è‡ªå·±çš„å®˜æ–¹è®ºå›ä¹Ÿæœ‰ä¸å°‘å¯ä»¥å‚è€ƒçš„èµ„æºï¼Œæ¯” repo é‡Œ issue æœ‰ä»·å€¼å¤šäº†ã€‚åœ°å€æ˜¯ meta.discourse.org.\n","date":"2019-10-21","permalink":"//localhost:1313/post/2019-10-21-30-min-discourse/","tags":["Self-host","Linux"],"title":"30 åˆ†é’Ÿ Discourse è®ºå›æ­å»ºç¬”è®°"},{"content":"SCP scp is always our friend.\nscp -i /path/to/permission/file username@ec2.url:/path/to/remote/file /path/to/local/directory  Update on 2019-10-14. What if I need to copy files that need root access?\nssh -i /path/to/permission/file username@ec2.url \u0026quot;sudo cat /var/log/nginx/access.log\u0026quot; \u0026gt; ~/Downloads/access.log  Generally, if it is a log that could be printed out. Just print and save it to local.\n","date":"2019-09-30","permalink":"//localhost:1313/post/2019-09-30-ec2-download/","tags":["EC2","AWS"],"title":"How to download files from AWS EC2"},{"content":"A colleague had a coding interview for Huawei last Sunday. I heard the second question was quite â€œmathematicalâ€. Let me rephrase it here a little bit.\n A hero summoner in a MOBA game has an ability to manipulate three elements. By controlling the order of releasing these elements, he can cast different spells accordingly. For example, casting in the order of fire, water, lightening can be treated as a spell. But there are some limitations as well.\n  Consider fitting the elements of that spell in a cycle. Then turning the cycle clockwise or counterclockwise does not produce any new spells. Additionally, inverting the cycle will not generate new ones either. The question is, if n is the number of elements he is capable of mastering, m is the number of elements consisting a spell, then what is the value of the number of different spells modulo 1000000007?\n Typically, the mathematical term that describes the way of ordering elements, is called Circular Permutation.\nThe number of ways to arrange $n$ distinct objects along a fixed (i.e., cannot be picked up out of the plane and turned over) circle is\n$$ P_n=(n-1)!$$  The reason why it is the factorial of \\(n-1\\) instead of $n$ is all cycle rotation.\nIf we consider a stricter definition, there will be only three free permutations (i.e., inequivalent when flipping the circle is allowed).\n$$Pâ€™_n=\\frac{1}{2} (n-1)!, n\\geq 3$$ In our problem, the number would be\n$${n \\choose m} \\frac{1}{2} (m-1)!$$ Since \\(1\\leq m \\leq 10000, 1\\leq n \\leq 20000\\) , direct calculation of factorial is suicidal for a computer. The hack here should be using modulo arithmetic, namely, we only keep the mod of \\(10^9 \u0026#43; 7\\) in intermediate steps.\nfact \u0026lt;- function(n) { res \u0026lt;- 1 for (i in 1:n) { res \u0026lt;- (res * i) %% 1000000007 } return(res) }  Although factorial(203) will give us Inf as a result, fact(203 wonâ€™t. It will give us an exact answer of 572421883.\n","date":"2019-09-10","permalink":"//localhost:1313/post/2019-09-10-circular-permutation/","tags":["Maths","Graph Theory"],"title":"Circular Permutation"},{"content":"On Windows, we have a package called installr. Use function copy.packages.between.libraries(), then problem solved.\nOn macOS, unfortunately, we donâ€™t have that handy tool.\nBut we can still use the following to retrieve all current installed packagesâ€™ names:\nto_install \u0026lt;- as.vector(installed.packages()[,1]) install.packages(to_install)  A more concrete solution would be only updating those non-base-R packages:\ninstalled \u0026lt;- as.data.frame(installed.packages()) write.csv(installed, 'installed_previously.csv') installedPreviously \u0026lt;- read.csv('installed_previously.csv') baseR \u0026lt;- as.data.frame(installed.packages()) toInstall \u0026lt;- setdiff(installedPreviously, baseR) install.packages(toInstall[,1])  Still, I wish those old packages can be transferred to a new version of R painlessly.\n","date":"2019-07-24","permalink":"//localhost:1313/post/2019-07-24-the-lost-r-packages/","tags":["R"],"title":"The Lost R Packages (after updating R)"},{"content":"Inspired by Alex Onsager and his wacky web app Pokemon Fusion, I was thinking about improving his approaches myself. In his post Pokemon Fusion: Behind the Scenes, he explains how he makes the head-swapping. Basically this can be included in three steps:\n Manually prepare separate parts (a head and a body) of a PokÃ©mon. Determine the face size. Determine the main palette of a PokÃ©mon. Transplant the head to its new owner and unify the torso color.   Introducing the King in the Soil: Dugking!\n The resizing part is definitely a highlight. But some weird combinations could be easily noticed due to the \u0026ldquo;manual separation\u0026rdquo;, such as this Magikarp head on a Gyarados body:\nNevertheless, it is some brilliant work done by Alex.\nCrawler Alex mentions he plans only to support the first 151 PokÃ©mons for his site. Understandable, because it\u0026rsquo;s really hard to automate the head/torso separation. To be honest, I really wish there could be some magical facical recognition algorithm for PokÃ©mons.\nThe first thing is to scrape the sprites of PokÃ©mons. My target website is PokÃ©mon Database which has a pretty organized gallery of all 809 PokÃ©mons from 7 generations.\nScraping with rvest is not the hard part. The only thing tricky worth mentioning is the set the sleeping time between batches. How do I know this? Learn from trials.\nt0 \u0026lt;- Sys.time() # do things here t1 \u0026lt;- Sys.time() Sys.sleep(0.5*as.numeric(t1-t0))   Sleeping time is proportional to response time.\n Color picker What\u0026rsquo;s next?\n Load the images of PokÃ©mon A and PokÃ©mon B. Use K-means to find the main palette of the two. Simply swap the colors of two palettes. In fact, we are swapping two vectors of clustering assignments. Reconstruct the PokÃ©mon A with main palette of PokÃ©mon B. (I forgot to make the other way around, sorry.)  At this point, I\u0026rsquo;ve realized that facial recognition with data so far is not practical since all images are only of size 30x40 pixels. In other words, the data is too limited.\nAnyways, there are still something I want to point out during the four steps above.\n Although the scraped images are in PNG format with transparent background, once loaded with load.image(), the RGB of these transparent pixels will \u0026ldquo;turn to the darkside\u0026rdquo;. I am pretty sure in most of the cases, the top two clusterings are the background \u0026ldquo;Mr Black\u0026rdquo; and some darkish grey boundary. You can alwasy check their sizes to confirm.  When swapping the palettes between two PokÃ©mons, we can just ignore these two unnecessary colors.   Due to the randomness in K-means (km()), the generated clusterings differ slightly from time to time.  The number of clusterings \\(k\\) I pick here is 7. Just do not set it too large.     Are Red Gyarados extremely rare? Maybe.\nBut ladies and gentlemen, I present you the Elite Four of Blue Magikarps!\nAnd four mediocore red worms. Meh!\nAs I mentioned above, the head swapping process cannot be fulfilled with this scale of data. The least I can do is to implement the color swapping. You can find my codes in the repo below. Have fun playing with it!\nGitHub\n","date":"2019-04-17","permalink":"//localhost:1313/post/2019-04-17-pokemon-recoloring/","tags":["Scraping","Visualization","k-means"],"title":"PokÃ©mon Recoloring"},{"content":"Updated on 2020-08-29: I\u0026rsquo;ve migrated my blog from blogdown to hugo so this post might be outdated. See the original post for examples.\nUpdated on 2019-09-08: Today I realized the best approach to insert a centered image inside a html-supported markdown post is:\n\u0026lt;img class=\u0026quot;special-img-class\u0026quot; style=\u0026quot;width:100%\u0026quot; src=\u0026quot;url-to-image\u0026quot; /\u0026gt;   Two common misunderstanding for longtime blogdown lurkers (like me) or R markdown newbies:\n  Blogging with blogdown \\( \\equiv \\) writing reports with knitr. There is no need to \u0026ldquo;knit\u0026rdquo; every post you write, no matter which format (.md or .Rmd) you are using, since blogdown will handle them itself.\n  Deploying HTML inside a R markdown file can make the blog post more expressive than merely using Rmd syntax.\n  I didn\u0026rsquo;t realize the second one until when I was browsing Nan Xiao\u0026rsquo;s blog, who is the author of this theme I\u0026rsquo;m using. At one point, I was wondering why his post looks so different from mine. Out of curiosity, I digged a little into his raw files and came up with these following \u0026ldquo;hacks\u0026rdquo;.\nInsert an image with style This has nothing to do with direct usage of HTML, but still is different from traditional markdown rendering. R markdown will not render the name/caption of an image automatically, but it\u0026rsquo;s not the same case in blogdown here.\nI used to like inserting an image with placeholder name like img, so the R markdown codes would look like this:\n![img](path-to-this-image)  Somehow if this is the case, blogdown will render the image without any problem but leave you a weird image caption \u0026ldquo;img\u0026rdquo;.\nSolution: Either you ignore the img with a pair of blank square brackets,\n![](path-to-this-image)  or you figure out a nice caption for the image and do the rest like before. And you can even insert a hyperlink inside the caption. For example:\n![This is a [logo](https://www.r-project.org/Rlogo.png) of R project.](https://www.r-project.org/Rlogo.png)  Update on 2019-04-17: You might also want to center images in the post. This can be done with a pair of \u0026lt;center\u0026gt;\u0026lt;/center\u0026gt; tags.\nBut somehow the caption disappears. Try this:\n\u0026lt;center\u0026gt; ![This is a [logo](https://www.r-project.org/Rlogo.png) of R project.](https://www.r-project.org/Rlogo.png) \u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;Logo here!\u0026quot;\u0026lt;/p\u0026gt; \u0026lt;/center\u0026gt;  Embed a YouTube video Here\u0026rsquo;s something R markdown cannot do. We need to borrow the power from HTML..\nUpdate on 2019-04-17: According to this post \u0026ldquo;Use shortcodes to embed tweets, videos, etc. in your blogdown blog posts \u0026rdquo;, one line code can solve the problem:\nblogdown::shortcode(\u0026quot;youtube\u0026quot;, \u0026quot;jKp-WuK6iv8\u0026quot;)  Note that in the code blog, we need to flag the argument eval=TRUE.\nAdditionally, I guess one line of HTML as caption \u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;How could this happen?\u0026quot;\u0026lt;/p\u0026gt; will increase the \u0026ldquo;stylelishness\u0026rdquo; of an embedded media by all means.\n\u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;How could this happen?\u0026quot;\u0026lt;/p\u0026gt;  Pre-formatted table R markdown tables look nice most of the time. But sometimes we need more. With html tag \u0026lt;pre\u0026gt;\u0026lt;/pre\u0026gt;, one can easily display a pre-formatted table.\n\u0026lt;pre\u0026gt;\u0026lt;output\u0026gt; item1 item2 item3 item4 ============================= Gou Li Guo Jia ----------------------------- Qi Yin Huo Fu ============================= run_time: 59.0s total_time: 60.0s \u0026lt;/output\u0026gt;\u0026lt;/pre\u0026gt;  A real example of WYSIWYG.\nCode display One way to do so is, of course, to write in a code block. Alternatively, we can attach an external link like this:\n\u0026lt;script src=\u0026quot;https://gist.github.com/rexarski/92e120c3ffad70877716d99f6bfd66a3.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  And finally this \u0026hellip; I thought the shadow under some inserted images come from some extra HTML scripts. The fact is that it\u0026rsquo;s a native feature in macOS.\n Command + Shift + 4: Take a screen shot with area selection, traditionally. Command + Shift + 5: Bring out the screenshot tool bar. If a screenshot is taken to capture the selected window, it will generate some shadow below.  ","date":"2019-04-16","permalink":"//localhost:1313/post/2019-04-16-rmd-cheatsheet/","tags":["Rmarkdown","R"],"title":"What a Rmd Cheatsheet Doesn't Tell Me"},{"content":"OpenStreetMap\u0026rsquo;s OSRM (Open Source Routing Machine) provides not only routing service, but a feature that enables drawing multiple routes to form a time-based polygon range as well. This is also called an isochrone.\nAn isochrone, according to Wikipedia, is defined as \u0026ldquo;a line drawn on a map connecting points at which something occurs or arrives at the same time\u0026rdquo;. In other words, it is just like contour lines measuring travel time in urban design.\nTimothÃ©e Giraud has a repo osrm that connects OpenStreetMap\u0026rsquo;s service and R.\nIn the README file of this repo, there is also a vanilla example of how to draw osrmIsochrone.\nlibrary(osrm) library(sp) library(cartography) library(leaflet) # Get isochones with a SpatialPointsDataFrame, custom breaks iso \u0026lt;- osrmIsochrone(loc = c(149.1212457, -35.2766747), breaks = seq(from = 0,to = 30, by = 5)) # Map osm \u0026lt;- getTiles(x = iso, crop = FALSE, type = \u0026quot;osm\u0026quot;, zoom = 13) tilesLayer(x = osm) bks \u0026lt;- sort(c(unique(iso$min), max(iso$max))) cols \u0026lt;- paste0(carto.pal(\u0026quot;turquoise.pal\u0026quot;, n1 = length(bks)-1), 80) choroLayer(spdf = iso, var = \u0026quot;center\u0026quot;, breaks = bks, border = NA, col = cols, legend.pos = \u0026quot;topleft\u0026quot;,legend.frame = TRUE, legend.title.txt = \u0026quot;Isochrones\\n(min)\u0026quot;, add = TRUE) plot(apotheke.sp[10,], add=TRUE, col =\u0026quot;red\u0026quot;, pch = 20)  However, the processing time of this plot is not very satisfying. What about drawing the polygons with leaflet?\n# Map as leaflet iso@data$drive_times \u0026lt;- factor(paste(iso@data$min, \u0026quot;to\u0026quot;, iso@data$max, \u0026quot;min\u0026quot;)) factpal \u0026lt;- colorFactor(\u0026quot;RdYlBu\u0026quot;, iso@data$drive_times) leaflet() %\u0026gt;% setView(149.1212457, -35.2766747, zoom = 11) %\u0026gt;% addProviderTiles(\u0026quot;CartoDB.Positron\u0026quot;, group=\u0026quot;Greyscale\u0026quot;) %\u0026gt;% addMarkers(lng = 149.1212457, lat = -35.2766747, popup = \u0026quot;Starting Point\u0026quot;) %\u0026gt;% addPolygons(fill=TRUE, stroke=TRUE, color = \u0026quot;black\u0026quot;, fillColor = ~factpal(iso@data$drive_times), weight=0.5, fillOpacity=0.2, data = iso, popup = iso@data$drive_times, group = \u0026quot;Drive Time\u0026quot;) %\u0026gt;% addLegend(\u0026quot;bottomright\u0026quot;, pal = factpal, values = iso@data$drive_time, title = \u0026quot;Drive Time\u0026quot;)  An interactive map is not a bad idea. And can use proc.time() to track the processing time of both methods.\nptm \u0026lt;- proc.time() # plotting method proc.time() - ptm  Compare the processing of two plotting above. osrm takes around 50 seconds, while leaflet() only requires less than 7 seconds.\n user system elapsed 28.081 13.457 48.232 user system elapsed 5.692 0.173 6.532  ","date":"2019-04-01","permalink":"//localhost:1313/post/2019-04-01-travel-time-polygon/","tags":["R","Visualization"],"title":"Travel Time Polygon with osrm"},{"content":"Though Reddit\u0026rsquo;s comment sorting system has been in use for almost a decade, the fact is that I wasn\u0026rsquo;t so exposed to western internet community till 2013, it is still, a rather new thing to me.\nRandall, the author of xkcd, also my favourite internet comics, wrote a blog1 explaining what the algorithm is. Meanwhile, a detailed version is introduced by Evan Miller2.\nIn the latter, Evan gave two examples of two \u0026ldquo;wrong\u0026rdquo; ranking methods:\n Score = positive - negative; Score = positive / total.  To say they are \u0026ldquo;wrong\u0026rdquo;, does not mean that they are not possible to give a rough idea in all scenarios. But the reality is always complex, a rough idea is probably a synonym of \u0026ldquo;meaninglessness\u0026rdquo;. To be specific, the first case ignore the \u0026ldquo;ratio\u0026rdquo; part in the term of \u0026ldquo;highest rated\u0026rdquo;, that means a more controversial comment might exceed a quality post simply due to more people voting on it. The second case, however, ignores the scenario where the sample space is limited. For instance, we can hardly say that a 1 of 1 upvoted comment is better than that of 99 of 100 upvoted comment.\nHow Reddit deals with this problem is trying to reach a confident balance between positive proportion and small number of observations. Now suppose the following:\n Each voting event is independent. Each event can either be a positive or a negative. The total number of votes is \\( n \\) , the number of positive votes is \\( k \\) , \\( \\hat{p}=k/n \\) is the positive proportion.  Now the idea is to first find each \\( \\hat{p} \\) , then calculate the corresponding confidence intervals, and finally rank the items by their lower bounds of confidence intervals.\nThe perfect expression of these confidence intervals here is:\n$$ \\left(\\hat{p}+\\frac{z^2_{\\alpha/2}}{2n} \\pm z_{\\alpha/2}\\sqrt{[\\hat{p}(1-\\hat{p})+z^2_{\\alpha/2}/4n]/n}\\right)/(1+z^2_{\\alpha/2}/n) $$\nIn 1928, mathematician Edwin Bidwell Wilson3 developed this score interval above to estimate the successful (or positive, in our case) probability \\( \\hat{p} \\) .\nHere\u0026rsquo;s my R script to simulate a comment ranking situation possibly happening every day on Reddit.\nlibrary(dplyr) Wilson \u0026lt;- function(n, k, alpha=0.95) { phat \u0026lt;- k/n score \u0026lt;- qnorm(1-(1-alpha)/2) lbound \u0026lt;- (phat+score**2/(2*n)-score*sqrt((phat*(1-phat)+score**2/(4*n))/n))/(1+score**2/n) return(lbound) } seed(2019) x \u0026lt;- sample(1:500,200,replace=T) y \u0026lt;- sample(1:500,200,replace=T) votes \u0026lt;- data.frame(x,y) votes %\u0026gt;% rowwise() %\u0026gt;% mutate(pos=min(x,y),total=max(x,y)) %\u0026gt;% select(total, pos) %\u0026gt;% mutate(Wilson=Wilson(total,pos)) %\u0026gt;% arrange(desc(Wilson))  And the pseudo-results are:\n# A tibble: 200 x 3 total pos Wilson \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; 1 473 467 0.973 2 494 487 0.971 3 390 385 0.970 4 234 232 0.969 5 354 347 0.960 6 237 233 0.957 7 123 121 0.943 8 391 376 0.938 9 296 285 0.935 10 415 397 0.932 # ... with 190 more rows    redditâ€™s new comment sorting system. \u0026#x21a9;\u0026#xfe0e;\n How Not To Sort By Average Rating. \u0026#x21a9;\u0026#xfe0e;\n Binomial proportion confidence interval. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-02","permalink":"//localhost:1313/post/2019-03-02-reddit-ranking/","tags":["Algorithm","Maths"],"title":"What does Top-Rated Mean in Redditâ€™s Ranking System"},{"content":"ä¸»è¦æ˜¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯ condaï¼Œä¸€ä¸ªæ˜¯ MySQL.\nå‡†å¤‡  åœ¨Vulträ¸Šè´­ç½®ä¸€å°æœåŠ¡å™¨ï¼Œé€‰æ‹©$5/æœˆçš„é…ç½®å°±å·²ç»å¤Ÿäº†ï¼Œç³»ç»Ÿé€‰æ‹© CentOSã€‚å½“ç„¶äº†ï¼ŒUbuntu ä¹Ÿæ˜¯æå¥½çš„ã€‚ ä½¿ç”¨sshè¿æ¥æœåŠ¡å™¨ï¼Œip åœ°å€ä»¥åŠä¸´æ—¶å¯†ç éƒ½åœ¨ Vultr çš„ dashboard é‡Œå¯ä»¥æŸ¥é˜…ã€‚  conda  å®‰è£…Anaconda distributionã€‚å› ä¸ºæœåŠ¡å™¨ç«¯æ²¡æœ‰å›¾å½¢ç•Œé¢ï¼Œä¸€åˆ‡éƒ½åœ¨æˆ‘ä»¬çš„ terminal é‡Œå®Œæˆã€‚åœ¨æœåŠ¡å™¨ä¸­è¿è¡Œwget https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh ä¹‹å‰æœåŠ¡å™¨é€‰æ‹©çš„æ˜¯ 64 ä½ç³»ç»Ÿï¼Œè¿™é‡Œå°±ä» conda çš„é¡µé¢æ‰¾åˆ°å¯¹åº”çš„ 64 ä½ç³»ç»Ÿä¸‹è½½åœ°å€ã€‚ ç”±äº CentOS æ²¡æœ‰bzip2ï¼Œæ­¤æ—¶ç›´æ¥å®‰è£…åˆ™ä¼šæŠ¥é”™ï¼Œåº”å…ˆä½¿ç”¨yum install bzip2å®‰è£…ã€‚ æ¥ç€ä½¿ç”¨bash Anaconda3-*.*.*-Linux-x86_64.shå®‰è£… anaconda. ä¾ç…§æç¤ºï¼Œéƒ½å¾ˆç›´ç™½ã€‚ é»˜è®¤å®‰è£…è·¯å¾„ä¼šåœ¨~ä¹‹ä¸‹ï¼Œå®‰è£…å®ŒæˆåæŠŠå®‰è£…è·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡é‡Œã€‚ å¦‚æœæ­¤æ—¶æ²¡æœ‰æ–‡æœ¬ç¼–è¾‘å™¨çš„è¯ï¼Œå»ºè®®yum install vimï¼Œä¸´æ—¶æŠ±ä½›è„šå­¦ä¹ ä½¿ç”¨ä¸€ä¸‹ã€‚ ä½¿ç”¨vim ~/.bashrcæ‰“å¼€ç¯å¢ƒå˜é‡å‚¨å­˜æ–‡ä»¶.bashrc. Vim ä¸»è¦æœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯æ’å…¥æ¨¡å¼ (insert mode)ï¼Œä¸€ç§æ˜¯æŒ‡ä»¤æ¨¡å¼ (command mode)ã€‚å½“å‰æˆ‘ä»¬å¤„äºæŒ‡ä»¤æ¨¡å¼ä¸‹ï¼ŒæŒ‰ié”®åˆ‡å…¥åˆ° insert modeï¼Œç„¶ååœ¨æ–‡ä»¶æœ«ç«¯å½•å…¥ä»£ç export PATH=â€œ$HOME/anaconda3/bin:$PATHâ€. æŒ‰Escé”®é€€å‡ºåˆ° command modeï¼Œå½•å…¥:wqä¿å­˜å¹¶é€€å‡ºï¼Œæ­¤æ—¶å…‰æ ‡ä¼šå‡ºç°åœ¨ vim çš„æœ€åä¸€è¡Œã€‚æ¨å‡ºä¹‹åï¼Œè®°å¾—source ~/.bashrcä¸€ä¸‹ã€‚ç„¶åæˆ‘ä»¬å°±å¯ä»¥ç”¨ conda çš„æŒ‡ä»¤æ¥æ£€æŸ¥ conda æ˜¯å¦æˆåŠŸå®‰è£…ï¼Œæˆ–è€…å®‰è£…å…¶ä»–çš„ packageã€‚ æ­¤æ—¶ conda çš„å®‰è£…å‘Šä¸€æ®µè½ï¼Œæ¥ä¸‹æ¥æ˜¯ MySQL æ•°æ®åº“çš„éƒ¨ç½²ã€‚  MySQL  åœ¨MySQL å®˜ç½‘å¯»æ‰¾ yum æºï¼Œç±»ä¼¼çš„æ–¹æ³•ï¼Œæ‰¾åˆ°é“¾æ¥å°±è¡Œã€‚è¿™é‡Œå› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯ CentOSï¼Œä½¿ç”¨ Red Hat Enterprise Linux 7 ç‰ˆæœ¬ã€‚yum localinstall https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm yum install mysql-community-server -y, å¹¶ç”¨mysqld --versionæŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸã€‚ é…ç½® MySQL ä½¿å…¶è‡ªåŠ¨å¯åŠ¨systemctl enable mysqld.service. å¯åŠ¨systemctl start mysqld.service. å¯¹äº 5.7 ä»¥ä¸Šç‰ˆæœ¬çš„ MySQLï¼Œè¿˜éœ€è¦ä»¥ä¸‹çš„æ–¹æ³•ä¿®æ”¹é»˜è®¤ root ç”¨æˆ·çš„ç™»å½•å¯†ç ã€‚ç¬¬ä¸€æ­¥æ˜¯å…ˆæ‰¾åˆ°ä¸´æ—¶å¯†ç ï¼šsudo grep 'temporary password' /var/log/mysqld.log mysql -u root -påæ¥ temporary password è¿›å…¥ MySQL. MySQL ç°åœ¨çš„å¯†ç è¦æ±‚è§„åˆ™æ¯”è¾ƒå¤æ‚ï¼Œå¦‚æœè‡ªå·±æƒ³è®¾ç½®ç®€å•å¯†ç ï¼Œåˆ™éœ€è¦SET GLOBAL validate_password.policy=LOW;. æœ€åä¿®æ”¹å¯†ç ALTER USER 'root'@'localhost' IDENTIFIED BY 'mynewpassword'; \\qé€€å‡º MySQL, exité€€å‡º ssh è¿æ¥ã€‚  åˆ°æ­¤ä¸ºæ­¢å°±åŸºæœ¬èƒ½ç”¨äº†ï¼Œç„¶åå°±æ˜¯ develop, play, and enjoy.\n","date":"2019-01-12","permalink":"//localhost:1313/post/2019-01-02-deploy-a-server-for-data-science/","tags":["conda","MySQL","Linux"],"title":"å¦‚ä½•éƒ¨ç½²ä¸€å°æ•°æ®ç§‘å­¦ä¸“ç”¨çš„ç®€æ˜“æœåŠ¡å™¨"},{"content":"I\u0026rsquo;ve always been wondering if Apple could utilize its collected health data in a more elegant way. The Activity app is decent, especially the achievements inside are definitely highlights in an iPhone x Apple Watch dynamic duo. On the other hand, the Health app serves nothing but a centralized information hub for various types data collected by either the sensor in your iPhone or your Apple Watch.\nWell, that\u0026rsquo;s basically my perspective and I don\u0026rsquo;t have any constructive suggestions yet. But one thing that always interests me is that if I can play some magic around those hidden data. The answer is an absolute yes. The data export from iPhone is quite easy, and kudos to Apple. The problem is, the data comes in JSON format. Ryan Praskievicz provided a Python script to convert those extracted JSON to csv, but also he attached a link to an online converter here.\nI initially had the very first model of Apple Watch, aka the \u0026ldquo;Apple Watch before Series 1\u0026rdquo;, aka \u0026ldquo;Series 0\u0026rdquo;, which should be considered as a crappy smart watch even back in a scenario 4 years ago. The battery life could hardly power through one day, and launching a third-party app could literally take half a minute. So the only two functions I used were time keeping and activaity monitoring. Unfortunately, the watch encountered a serious this May and I still haven\u0026rsquo;t figured out what happened to it. But in short, the watch stopped syncing with my iPhone and did not track any movements either. I had to perform a hard reset and thus lost all data prior to that day. So far, the data sitting on my MacBook is my Apple Watch health data collected from early this May to October.\nThe size of the data is around 90 mb and it wouldn\u0026rsquo;t take too long to convert them to csv. But this really depends. I mean if you have a load of several years\u0026rsquo; data, I strongly suggest you make yourself a cup of tea during this time. I haven\u0026rsquo;t checked the data structure inside at that moment, but a vague goal is to mimic the \u0026ldquo;major three\u0026rdquo; on Apple Watch: the activities, the exercise time and the standing hours. Surprisingly, the exported data has no standing hours, so that I have to use daily steps as a substitute.\nlibrary(tidyverse) library(lubridate) library(gridExtra) library(RColorBrewer) read_one_field \u0026lt;- function(filename) { file \u0026lt;- read_delim(paste0(\u0026quot;./\u0026quot;, filename), delim=\u0026quot;;\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;)) # glimpse(file) if (str_detect(file$type[1], regex(\u0026quot;ActiveEnergyBurned\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: kcal mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(active = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(active = sum(active)) } else if (str_detect(file$type[1], regex(\u0026quot;AppleExerciseTime\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: min mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(exercise = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(exercise = sum(exercise)) } else if (str_detect(file$type[1], regex(\u0026quot;StepCount\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: count mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(step = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(step = sum(step)) } } active \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierActiveEnergyBurned.csv\u0026quot;) exercise \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierAppleExerciseTime.csv\u0026quot;) step \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierStepCount.csv\u0026quot;)  The csv files are pretty neat overall, only a few NAs to deal with. Afterwards, I notice that each entry is responsible for an time elapse ranging from several seconds to minutes. Hence aggregation is unavoidable. The most natural way to do so is to group them by days.\n# standard: active = 400, exercise = 30, step = 10000 full_data \u0026lt;- active %\u0026gt;% full_join(exercise) %\u0026gt;% full_join(step) %\u0026gt;% replace_na(list(active = 0, exercise = 0, step = 0)) %\u0026gt;% mutate(active = active / 400, exercise = exercise / 30, step = step / 10000) %\u0026gt;% gather(key = type, value = value, -day)  Soon I come up with a line plot using a cutomized theme, which seems pretty similar to Apple\u0026rsquo;s Activity color palette. The theme is based on a pure black ggplot2 theme on GitHub.\ntheme_black = function(base_size = 12, base_family = \u0026quot;\u0026quot;) { theme_grey(base_size = base_size, base_family = base_family) %+replace% theme( # Specify axis options axis.line = element_blank(), axis.text.x = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;, lineheight = 0.9), axis.text.y = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;, lineheight = 0.9), axis.ticks = element_line(color = \u0026quot;white\u0026quot;, size = 0.2), axis.title.x = element_text(size = base_size, color = \u0026quot;white\u0026quot;, margin = margin(0, 10, 0, 0)), axis.title.y = element_text(size = base_size, color = \u0026quot;white\u0026quot;, angle = 90, margin = margin(0, 10, 0, 0)), axis.ticks.length = unit(0.3, \u0026quot;lines\u0026quot;), # Specify legend options legend.background = element_rect(color = NA, fill = \u0026quot;black\u0026quot;), legend.key = element_rect(color = \u0026quot;white\u0026quot;, fill = \u0026quot;black\u0026quot;), legend.key.size = unit(1.2, \u0026quot;lines\u0026quot;), legend.key.height = NULL, legend.key.width = NULL, legend.text = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;), legend.title = element_text(size = base_size*0.8, face = \u0026quot;bold\u0026quot;, hjust = 0, color = \u0026quot;white\u0026quot;), legend.position = \u0026quot;right\u0026quot;, legend.text.align = NULL, legend.title.align = NULL, legend.direction = \u0026quot;vertical\u0026quot;, legend.box = NULL, # Specify panel options panel.background = element_rect(fill = \u0026quot;black\u0026quot;, color = NA), panel.border = element_rect(fill = NA, color = \u0026quot;white\u0026quot;), panel.grid.major = element_line(color = \u0026quot;grey35\u0026quot;), panel.grid.minor = element_line(color = \u0026quot;grey20\u0026quot;), panel.margin = unit(0.5, \u0026quot;lines\u0026quot;), # Specify facetting options strip.background = element_rect(fill = \u0026quot;grey30\u0026quot;, color = \u0026quot;grey10\u0026quot;), strip.text.x = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;), strip.text.y = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;,angle = -90), # Specify plot options plot.background = element_rect(color = \u0026quot;black\u0026quot;, fill = \u0026quot;black\u0026quot;), plot.title = element_text(size = base_size*1.2, color = \u0026quot;white\u0026quot;), plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), \u0026quot;cm\u0026quot;) ) }  ggplot(full_data, aes(x=day, y=value)) + geom_line(aes(color=type),size=1) + scale_color_manual(labels = c(\u0026quot;Active Energy\u0026quot;, \u0026quot;Exercise Time\u0026quot;, \u0026quot;Steps\u0026quot;), values=c(\u0026quot;#FF0054\u0026quot;, \u0026quot;#8CFF00\u0026quot;, \u0026quot;#00FFCA\u0026quot;)) + theme_black() + facet_grid(type ~ .) + labs(title=\u0026quot;Apple Watch Health Data\u0026quot;, subtitle=\u0026quot;what to write here\u0026quot;, caption=\u0026quot;Source: my retired Apple Watch (1st gen)\u0026quot;, y=\u0026quot;Complete 100%\u0026quot;, color=NULL)  But I\u0026rsquo;m not satisfied with theme somehow, so I decide to experiment a little bit on a FiveThirtyEight-style theme. It is a more furnished theme, but does not match the Activity app palette. Well, I just cannot reject the magic of minimalism.\nfte_theme \u0026lt;- function() { # Generate the colors for the chart procedurally with RColorBrewer palette \u0026lt;- brewer.pal(\u0026quot;Greys\u0026quot;, n=9) color.background = palette[2] color.grid.major = palette[3] color.axis.text = palette[6] color.axis.title = palette[7] color.title = palette[9] # Begin construction of chart theme_bw(base_size=9) + # Set the entire chart region to a light gray color theme(panel.background=element_rect(fill=color.background, color=color.background)) + theme(plot.background=element_rect(fill=color.background, color=color.background)) + theme(panel.border=element_rect(color=color.background)) + # Format the grid theme(panel.grid.major=element_line(color=color.grid.major,size=.25)) + theme(panel.grid.minor=element_blank()) + theme(axis.ticks=element_blank()) + # Format the legend, but hide by default theme(legend.position=\u0026quot;none\u0026quot;) + theme(legend.background = element_rect(fill=color.background)) + theme(legend.text = element_text(size=7,color=color.axis.title)) + # Set title and axis labels, and format these and tick marks theme(plot.title=element_text(color=color.title, size=10, vjust=1.25)) + theme(axis.text.x=element_text(size=7,color=color.axis.text)) + theme(axis.text.y=element_text(size=7,color=color.axis.text)) + theme(axis.title.x=element_text(size=8,color=color.axis.title, vjust=0)) + theme(axis.title.y=element_text(size=8,color=color.axis.title, vjust=1.25)) + # Plot margins theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), \u0026quot;cm\u0026quot;)) }  ggplot(full_data, aes(x=day, y=value)) + geom_line(aes(color=type),size=1) + fte_theme() + facet_grid(type ~ .) + labs(title=\u0026quot;Apple Watch Health Data\u0026quot;, subtitle=\u0026quot;what to write here\u0026quot;, caption=\u0026quot;Source: my retired Apple Watch (1st gen)\u0026quot;, y=\u0026quot;Complete 100%\u0026quot;, color=NULL)  But even now I\u0026rsquo;m not so sure about what I can really do with these data, except for probably plotting them with lines and dots. Meanwhile, I\u0026rsquo;m not okay with the variable on y-axis as well. I tried to combine them in one plot at the very beginning, but it turned out that the three types of data were not on the same scale. So I made them the percentages of the completeness of daily goals by setting activity to 400, exercise time to 30 minutes and steps to 10,000. But still, the y-axis should better be \u0026ldquo;the ratio to the goal\u0026rdquo; instead of a bunch of misleading integers (1%? 2%?)\nWell, at least I can conclude that all these match the title which says \u0026ldquo;a sneak peek\u0026rdquo;, not a thorough digging.\n","date":"2018-11-26","permalink":"//localhost:1313/post/2018-11-26-apple-watch-health-data/","tags":["iOS","ggplot2","Visualization"],"title":"A Sneak Peek at Apple Watch Health Data"},{"content":"So today marks the release date of the latest Hearthstone expansion Dr. Boom\u0026rsquo;s Project The Boomsday Project. As usual, I\u0026rsquo;ve done some pack opening with my pal. But this time, I\u0026rsquo;ve also taken down some data and made a simple scatterplot (maybe?) just for fun.\nSome results  I was probably too sleepy at 3 am in the morning, lost 2 packs\u0026rsquo; data. The total pack number should be 120 instead of 118. Including the guaranteed one in the first 10 packs, I discovered 9 legendary cards in total. But unfortunately, none of them is golden. The first 8 legendary cards were opened in the first 60 packs. Pretty imbalanced! The most common card combination is, of course, 4 commons + 1 rare (Let\u0026rsquo;s call it \u0026ldquo;4c1r\u0026rdquo;). Out of all my 118 packs, 66 of them are of this type, which take about 56% of total pack opening. (Blizzard you are too greedy!) The longest streak of 4c1r is 6.  This post might not have any actual meaning. But I\u0026rsquo;m generally happy with the results since at least I\u0026rsquo;ve got the leading role of this expansion: Dr. Boom.\n","date":"2018-08-08","permalink":"//localhost:1313/post/2018-08-08-hearthstone/","tags":["R","Visualization","ggplot2"],"title":"Visualizing Hearthstone Pack Opening with Animation"},{"content":"The post here demonstrates an example of Hacker News scraper with rvest library.\nBut a tiny problem emerges as sometimes YC-funded startup post job ads on the front page so that the scraper would find different information vector with different lengths. Specifically, the it would not return any score value for that ad. One possible remedy is to log in as a real user, then do the scraping. In R, it is implemented as:\nlibrary(rvest) login \u0026lt;- 'https://news.ycombinator.com/login' session \u0026lt;- html_session(login) form \u0026lt;- html_form(session)[[1]] filled_form \u0026lt;- set_values(form, acct='MyAccountName', pw='MyPassword') submit_form(session, filled_form) content \u0026lt;- jump_to(session, 'https://news.ycombinator.com/') title \u0026lt;- content %\u0026gt;% html_nodes('a.storylink') %\u0026gt;% html_text() link_domain \u0026lt;- content %\u0026gt;% html_nodes('span.sitestr') %\u0026gt;% html_text() score \u0026lt;- content %\u0026gt;% html_nodes('span.score') %\u0026gt;% html_text() age \u0026lt;- content %\u0026gt;% html_nodes('span.age') %\u0026gt;% html_text() df \u0026lt;- data.frame(title = title, link_domain = link_domain, score = score, age = age)  The saved data frame would contain all the 30 links on the front page of Hacker News.\n","date":"2018-06-26","permalink":"//localhost:1313/post/2018-06-26-basic-web-scraping-with-rvest/","tags":["Scraping","R"],"title":"Basic Web Scraping with rvest"},{"content":"The design was inspired by a blog on Fronkonstin: Experiments in R. I modified the code a little bit, so that the output is generated by Peter de Jong Attractors instead of Clifford Attractors.\nThe definitive expressions of Peter de Jong Attractors are shown below:\n$$ \\begin{aligned} x_{n+1} \u0026amp;= \\sin(a \\cdot y_n) - \\cos(b \\cdot x_n) \\cr y_{n+1} \u0026amp;= \\sin(c \\cdot x_n) - \\cos(d \\cdot y_n) \\end{aligned} $$\nCodes:\n The output looks really pretty good, like a basket. You can visit this link for more examples and detailed parameters.\n","date":"2018-01-08","permalink":"//localhost:1313/post/2018-01-08-de-jong-attractors/","tags":["Maths","Generative"],"title":"Drawing Trajectory of Peter de Jong Attractors with ggplot2"},{"content":"How to pick more beautiful colors for your data visualizations:\n You have lots of options. Which means you can stay in a small area of the color wheel and still have many options. Which means:\nDonâ€™t dance all over the color wheel.\n This is a perfect starting tutorial for people, who knows some superficial knowledge about design and eagerly want to improve his/her aesthetic sense, like me.ğŸ™Œ\n","date":"2020-09-08","permalink":"//localhost:1313/post/2020-09-08-beautifulcolors/","tags":["Visualization","Design"],"title":"Color Selection in Data Visualization"},{"content":"Edit: According to Miniflux\u0026rsquo;s documentation, refreshing feeds is not possible with Reeder since no user information is sent. This means, I can only fetch something new when the server side refreshes (by default, it\u0026rsquo;s once an hour), no matter what syncing frequency I set in the application. That\u0026rsquo;s definitely a bummer. But I will keep an eye on this. Maybe it will eventually become a turning point when I finally quit information overdosing.\n Oceans rise, empires fall. RSS will be back.\n My subscription to Inoreader will expire soon. It\u0026rsquo;s been a good year with it, but it was in fact less useful than I expected. From a retrospective point of view, the possibility to subscribe to more than 150 feeds might be the only feature that still seems attractive to me. But hey, why not more than 250, or 500? Although at the same time, I truly believe in the philosophy of \u0026ldquo;less is more\u0026rdquo; and \u0026ldquo;I probably won\u0026rsquo;t have enough time for that\u0026rdquo;.\nSo, three steps to unleash the true power of a personal RSS subscription service.\n Set up a Ubuntu server. (Optional) Deploy RSSHub on that server. Deploy Miniflux on that server.  Set Up A New Ubuntu Server I\u0026rsquo;m going to skip some introduction and simply paste the script here. Just follow Docker\u0026rsquo;s documentation to install docker and docker-compose on Ubuntu.\napt-get update apt-get dist-upgrade -y update-alternatives --config editor # set up docker repository sudo apt-get remove docker docker-engine docker.io containerd runc sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common # Add Dockerâ€™s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\u0026quot; # install docker engine sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io sudo docker run hello-world # install docker-compose sudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # if failed, create a symbolic link # sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose # verified docker-compose --version  Deploy RSSHub RSSHub is an open source RSS feed aggregator.\n# docker compose deployment wget https://raw.githubusercontent.com/DIYgod/RSSHub/master/docker-compose.yml docker volume create redis-data docker-compose up -d # update by removing old containers # docker-compose down # docker deployment docker pull diygod/rsshub docker run -d --name rsshub -p 1200:1200 diygod/rsshub # stop container # docker stop rsshub # docker rm rsshub # then repeat installation  Deploy Miniflux After reading a post from Luke Singham, I\u0026rsquo;ve decided pick Miniflux as my self-hosted RSS service. It\u0026rsquo;s simple, looks clean, and easy to deploy.\nmkdir miniflux cd miniflux vim docker-compose.yml  Remember to keep a record of both pairs of usernames and passwords.\nversion: \u0026quot;3\u0026quot; services: miniflux: image: miniflux/miniflux:latest ports: - \u0026quot;80:8080\u0026quot; depends_on: - db environment: - DATABASE_URL=postgres://db_username:db_password@db/miniflux?sslmode=disable - RUN_MIGRATIONS=1 - CREATE_ADMIN=1 - ADMIN_USERNAME=miniflux_username - ADMIN_PASSWORD=miniflux_password db: image: postgres:latest environment: - POSTGRES_USER=db_username - POSTGRES_PASSWORD=db_password volumes: - miniflux-db:/var/lib/postgresql/data volumes: miniflux-db:  docker-compose up -d db docker-compose up -d miniflux  So now I can access the Miniflux service by going to my ip address. Input the Miniflux username and password, then we are all good. In addition, I need to enable Fever api in Settings in order to read my feeds in other third party applications like Reeder.\nNote: the \u0026ldquo;server\u0026rdquo; blank to fill in should be my ip/url + /fever.\n","date":"2020-08-29","permalink":"//localhost:1313/post/2020-08-29-self-host-rss/","tags":["RSS","Self-host"],"title":"Self-host RSS"},{"content":"This post is a subsequent blog to my daily experience basically in the last two months. As the general situation of the COVID-19 pandemic in Australian Capital Territory keeps is keeping stable, sometimes I almost forget to check on this Shortcut.\nLong story short, general idea is if you have very specific information in need, you can always create a temporary iOS Shortcut to assist you in capturing such information. In my case, it is the number of confirmed cases in ACT. Very specific. Therefore, all I need to do is to go to the governmentâ€™s website, find the page containing that value, and scrape it in HTML. And BOOM, you get it out of there.\nIâ€™ll specify the steps in the context of Shortcut:\n Create an action called â€œURLâ€. Insert the value as https://www.health.act.gov.au/about-our-health-system/novel-coronavirus-covid-19. Create an action called â€œGet Contents of URLâ€ with method GET. Create an action called â€œMake rich text from HTMLâ€. Replace HTML with Contents of URL. Create an action called â€œMatch Textâ€. Replace the two placeholders with (Confirmed cases \\d+) and Rich Text from HTML respectively. Create one last action called â€œShow resultsâ€. And change the last variable to Matches.   iCloud link to this Shortcut.\n","date":"2020-06-14","permalink":"//localhost:1313/post/2020-06-14-covid-shortcut/","tags":["iOS","Shortcut"],"title":"COVID confirmed cases Shortcut"},{"content":"99 days ago, at some moment on that day, I told myself â€œhey bro, itâ€™s time to kiss goodbye to those old residents crawling in your read-it-later-but-never-know-when-ish reading list and something like thatâ€.\nSo with that kept in mind, I have to jot down the number of items left in each categories like obsessive-compulsory disorder in Notion. Shout-out to Notion, it\u0026rsquo;s an amazing workspace for most of daily work. Back to the business, but I do really guess itâ€™s okay, 99 days have passed just in a flash. So I spent another 10 minutes this afternoon to replay this process in R.\nThe reproducible R code is attached in Gist.\n As you can see, in this 99 days of period, I actually cleaned up some to-dos like magic. I mean they are gone for good, hopefully I will not need to deal with them again. Therefore no chance of procrastination for them. On the other hand, I still have some increasing trend of hoarding to-watch videos on YouTube, which I have to point out that I only keep them for test preparation next month so that might not be as representative as it seems.\nAnywho, the best part of this decluttering project (Iâ€™d like to call that, sounds much cooler) is not only the Iâ€™ve cleaned up of my digital life (mostly), but I believe Iâ€™ve achieved some state of tranquility as well. That is to say, no matter how many unread articles in my RSS reader, no matter how many new episodes of my subscribed podcasts were released in the last 24 hours, and apparently no matter how many marvelous contents are there just existing on the Internet, I donâ€™t need to care about them at all.\nI ainâ€™t superman and I donâ€™t have unlimited concentration every day, I just donâ€™t need to give a damn about what is really going on in every details. Iâ€™d happily click a button to â€œread allâ€, â€œarchive allâ€ or â€œdelete allâ€.\nI donâ€™t wanna be overwhelmed by probably-not-that-useful miscellaneous stuffs.\nAnd happy 27th birthday to me.\n","date":"2020-06-13","permalink":"//localhost:1313/post/2020-06-13-decluttering/","tags":["Productivity","R"],"title":"99 days of Decluttering"},{"content":"Devon æœ¬æ„æ˜¯è‹±å›½çš„å¾·æ–‡éƒ¡ï¼Œç”±äºæ­¤åœ°çš„åœ°å±‚æœ€æ—©è¢«ç ”ç©¶ï¼Œä¾¿ç”¨ Devonian ç§°å‘¼åœ¨æ­¤å¤„å‘ç°çš„ç‰¹åœ°åœ°å±‚ã€‚å°”åç¿»è¯‘ä¸ºä¸­æ–‡ä¾¿å”¤åšã€Œæ³¥ç›†çºªã€ã€‚\né‚£ä¹ˆ DEVONthink 3 æ˜¯æˆ‘åœ¨ macOS ä¸Šå¸¸ç”¨çš„æ–‡æ¡£ç®¡ç†å·¥å…·ï¼ˆè™½ç„¶å®ƒèƒ½å¤Ÿå‘æŒ¥çš„ä½œç”¨è¿œå¤§äºæ­¤ï¼‰ï¼Œä½†è‡ªå·±å…¶å®å¹¶æ²¡æœ‰å¼„æ¸…æ¥šå®ƒçš„å¤‡ä»½ä¸åŒæ­¥åŠŸèƒ½ï¼Œå°¤å…¶æ˜¯å’Œå®ƒè‡ªå·±çš„ iOS ç‰ˆæœ¬ app ä¹‹é—´çš„å…³ç³»æ›´æ˜¯ä¸€å¤´é›¾æ°´ã€‚å¶ç„¶ä¹‹é—´ä»Šå¤©å‘ç°æœ¬åœ° iCloud Drive æ–‡ä»¶å¤¹è™½ç„¶åªæœ‰å¤§çº¦å°å‡ ç™¾ mb çš„æ–‡ä»¶ï¼Œä½†ä» ~/Library/Mobile Documents è·¯å¾„è®¿é—®åˆ™ä¼šçœ‹åˆ°ä»¤äººå‘æŒ‡çš„ 30gb æ–‡ä»¶å¤§å°ã€‚è€Œä¸”éå¸¸ç¡®å®šçš„æ˜¯ï¼Œè¿™äº›æ–‡ä»¶æ—¢å ç”¨äº†æœ¬åœ°ç¡¬ç›˜ï¼Œåˆå ç”¨äº† iCloud Drive å­˜å‚¨ç©ºé—´ï¼Œè€Œä¸”è¿˜å¹¶æ²¡æœ‰åº”æœ‰çš„ä½œç”¨ã€‚äºæ˜¯ä¹ï¼Œæœ¬ç€æ•‘ç¡¬ç›˜äºæ°´ç«çš„æƒ³æ³•ï¼Œæƒ³è¦æŠŠå†—ä½™çš„å¤‡ä»½æ¸…æ‰«å¹²å‡€ã€‚\né¦–å…ˆè¦åšçš„äº‹æƒ…æ˜¯ï¼Œåœ¨ macOS ç‰ˆæœ¬çš„ DEVONthink ä¸­å°†åŸæœ‰çš„ sync location åå‘å‹¾é€‰ iCloud è¿™ä¸€é¡¹ã€‚ï¼ˆæœ¬åœ°å¤‡ä»½è¿˜æ˜¯æ¯”è¾ƒæ˜æœ—çš„ï¼Œæ’ä¸€å—å¤–æ¥ç¡¬ç›˜ï¼Œé€‰æ‹©æƒ³è¦å¤‡ä»½çš„ databases å°±ç®—å®Œå·¥äº†ã€‚è€Œä¸”å¤‡ä»½æ–‡ä»¶å¯ä»¥ç›´æ¥é€šè¿‡ Finder è®¿é—®ï¼Œè¦æ€è¦å‰ä»»å›ä½¿å”¤ã€‚ï¼‰\nç„¶åè¦åšçš„å°±æ˜¯å» iCloud ä¸­æ‰¾åˆ° DEVONthink å¤‡ä»½æ‰€å ç”¨çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚æ­¤å¤„æœ‰ä¸¤ç§æ–¹æ³•ï¼š\n ç¬¬ä¸€ç§æ–¹æ³•ä»…é™ç”¨äºç”¨æˆ·å†å²ä»¥æ¥åªé€šè¿‡ iCloud å¤‡ä»½è¿‡ä¸€æ¬¡æ•°æ®çš„æƒ…å†µä¸‹ã€‚é€šè¿‡ iOS çš„ç³»ç»Ÿè®¾ç½® Settings - Apple ID - iCloud - Manage Storage - DEVONthink To Go - Delete Data å°±å¯ä»¥å®Œæˆã€‚ä½†å®é™…æƒ…å†µå¾€å¾€ä¸æ˜¯è¿™æ ·ï¼Œåœ¨å…ˆå‰çš„æ— æ•°æ¬¡å°è¯•ä¸­ï¼Œæˆ–å¤šæˆ–å°‘æˆ‘å·²ç»å°è¯•è®¾ç½®è¿‡å¤šæ¬¡ä»¥ iCloud ä¸ºå¤‡ä»½åœ°å€ï¼Œé‚£ä¹ˆå®é™…ä¸Š iCloud ä¸­å°±ä¼šåœ¨ä¸€ä¸ªè·¯å¾„ä¸‹å­˜æ”¾æœ‰å¤šæ¬¡å¤‡ä»½æ–‡ä»¶ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘åªæ˜¯å¤‡ä»½äº† Inbox å’Œå¦å¤–ä¸€ä¸ªå° databaseï¼Œä½†æ€»çš„å¤‡ä»½æ–‡ä»¶æ‰€å ç©ºé—´æœ‰ 30gbã€‚å…·ä½“è€Œè¨€ï¼Œå°±æ˜¯å…ˆå‰æˆ‘ä¸€å®šå¤‡ä»½è¿‡æ›´å¤šçš„æ•°æ®ï¼Œè€Œå¤‡ä»½çš„æ®‹éª¸éƒ½ç•™å­˜åœ¨äº† iCloud çš„è¿™ä¸ªå¤‡ä»½è·¯å¾„ä¸‹ã€‚ ç¬¬äºŒç§æ–¹æ³•ï¼Œå°±æ˜¯ç›´æ¥æ€åˆ° iCloud é‡Œå¹²æ‰æ•´ä¸ªå¤‡ä»½æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæƒ³ç›´æ¥é€šè¿‡ Finder è®¿é—®ï¼Œä½ å¤šåŠåªæ˜¯çœ‹åˆ°äº† iCloud Drive é‡Œé›¶é›¶æ˜Ÿæ˜Ÿçš„ä¸€äº›æ–‡ä»¶ã€‚è¦çŸ¥é“å•Šï¼ŒiCloud Drive åªæ˜¯ iCloud çš„ä¸€ä¸ªå­é›†ã€‚æ‰€ä»¥æ ¸å¿ƒé—®é¢˜å°±åœ¨äºï¼Œæˆ‘æƒ³å»çœ‹å­˜åœ¨ iCloud é‡Œçš„ DEVONthink å¤‡ä»½è·¯å¾„ï¼Œä¸ºä»€ä¹ˆä½ éæŠŠæˆ‘å¾€æ”¾æ–‡ä»¶çš„è·¯å¾„å¸¦å‘¢ï¼Ÿç»ˆç«¯èµ°èµ·ã€‚  cd ~/Library/Mobile\\ Documents ls rm -rf iCloud~679S2QUWR8~com~devon-technologies~sync  âš ï¸ æ³¨æ„ï¼š åœ¨ rm -rf ä¹‹å‰è¿˜æ˜¯ ls ç¡®è®¤ä¸€ä¸‹è¿™ä¸ªå¤‡ä»½è·¯å¾„ç¡®å®å­˜åœ¨ï¼ŒåŒ…å« com-devon-technologies çš„å…³é”®è¯ï¼Œç„¶åç”¨ sudo æƒé™æ¸…ç†å¹²å‡€ã€‚\næ­¤æ—¶ï¼Œæˆ‘ä¸ç¦è”æƒ³åˆ°ï¼Œé‚£æˆ‘æ›¾ç»å°è¯•è¿‡çš„åˆ«çš„ sync locations ä¼šä¸ä¼šä¹Ÿæœ‰åŒæ ·çš„ã€Œéšè—å¤‡ä»½æ–‡ä»¶ã€çš„é—®é¢˜å‘¢ï¼Ÿäºæ˜¯æˆ‘å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œé‡æ–°æ·»åŠ äº† Dropbox ä½œä¸ºå¤‡ä»½åœ°å€ã€‚å¤‡ä»½æ•°æ®åº“æˆ‘é€‰æ‹©äº†åªæœ‰ä¸‰ä¸ªæ–‡ä»¶çš„ Global Inboxï¼Œæ–¹ä¾¿è¯•é”™ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æ–°å»ºå¤‡ä»½åç§°çš„æ—¶å€™ DEVONthink å±…ç„¶ç»™æˆ‘è‡ªåŠ¨è”æƒ³äº†ä¸¤ä¸ªæˆ‘æ›¾ç»çš„å¤‡ä»½åç§°ï¼æˆ‘æœ¬ä»¥ä¸ºå½“æ—¶åœ¨ DEVONthink çš„è®¾ç½®é‡Œåˆ é™¤æ‰è¿™ä¸ªå¤‡ä»½è·¯å¾„ï¼Œå°±è‡ªåŠ¨æŠŠå¤‡ä»½è·¯å¾„é‡Œçš„å¤‡ä»½æ–‡ä»¶ä¹Ÿåˆ é™¤äº†ï¼Œç»“æœå¹¶ä¸æ˜¯è¿™æ ·ã€‚æˆ‘è¯•å›¾ä» Dropbox çš„æ¡Œé¢å®¢æˆ·ç«¯è®¿é—®åˆ° Apps æ–‡ä»¶å¤¹ï¼Œçœ‹ä¸€çœ‹ DEVONthink æ˜¯å¦çœŸçš„æœ‰å‡ ä¸ªå¤‡ä»½æ–‡ä»¶å­˜åœ¨è¿™é‡Œï¼Œç»“æœå‘ç°è¿™ä¸ªæ–‡ä»¶å¤¹ä½œä¸ºä¸ Dropbox å…³è”çš„ Linked Appï¼Œå¹¶æ²¡æœ‰å‡ºç°åœ¨æœ¬åœ°ã€‚åˆ‡æ¢åˆ°ç½‘é¡µç«¯ï¼Œå´çœŸçš„å‘ç°äº†æ›¾ç»çš„ä¸¤ä¸ªå¤‡ä»½æ–‡ä»¶éƒ½é™é™åœ°èººåœ¨ Dropbox çš„å­˜å‚¨ç©ºé—´é‡Œã€‚æœæ–­é€‰æ‹©äº†åˆ é™¤ï¼Œäºæ˜¯ Dropbox çš„ç©ºé—´ä¹Ÿå¾—åˆ°äº†é‡Šæ”¾ã€‚\næ‰€ä»¥ï¼Œæ€»ç»“ä¸€ä¸‹ä»Šå¤©çš„å‘ç°ï¼š\nåœ¨ DEVONthink é‡Œåˆ é™¤ sync locations å¹¶ä¸ç­‰åŒäºåˆ é™¤å·²ç»åˆ›å»ºçš„å¤‡ä»½æ–‡ä»¶ã€‚å¦‚æœéœ€è¦åˆ é™¤å¤‡ä»½æ–‡ä»¶ï¼Œéœ€è¦åˆ°æŒ‡å®šå¤‡ä»½åœ°å€çš„ explicit location å»å¼ºè¡Œåˆ é™¤è¿™äº›å¤‡ä»½æ–‡ä»¶ï¼›å¦åˆ™åˆ™ä¼šæœ‰å¾ˆå¤šç©ºé—´è¢«å·å·å ç”¨ï¼Œæ ¹æœ¬ä¹Ÿæ— ä»çŸ¥æ™“ã€‚\n","date":"2020-06-09","permalink":"//localhost:1313/post/2020-06-09-devonthink-backup-cleanup/","tags":["macOS","Productivity"],"title":"DEVONthink å¤‡ä»½æ¸…æ‰«"},{"content":"As a fellow redditor, I can naturally tell you what is trending on the /r/dataisbeautiful subreddit. Beautiful (or maybe not) charts come and go. One or two years back, there were ridge plots aka joy plots. Then starting from March 2019, the bar chart races swiped the the subreddit and YouTube. I tried to take a dip since then but could not finish one before its moratorium.\nStill, it is a fun chart and its â€œracingâ€ style definitely outplays any regular bar charts. So even being late to the party, I would like to try it on my own. By saying â€œon my ownâ€, I actually mean â€œreplicating othersâ€™ workâ€ and make some changes to see whatâ€™s going on.\n And yes there are lots of problems in this racing bar chart, for example, the subtitle is so closed to title, the color palette could be more carefully picked etc. But Iâ€™m just gonna leave it here because this is the â€œsmall dipâ€ I was talking about. Not diving in too deep.\n","date":"2020-06-05","permalink":"//localhost:1313/post/2020-06-05-late-party/","tags":["R","Visualization"],"title":"Late to the party"},{"content":"Inspired by Stefanâ€™s blog post, I set up my own photo stream as well. Itâ€™s called Air Avo, a self-hosted photo scream based on a repo by Tim Van Damme.\nA bunch of funny things need to point out:\n Modify photo-stream/_includes/head.html to change the favicon. Modify photo-stream/index.html to change the link in the right bottom corner of the stream page. I airdropped some photos from my iPhone to my MacBook. Some of them share an extension of .JPG instead of .jpg. Such photos wonâ€™t be rendered in the build. I manually change them back to .jpg but Git thinks they are literally the same. The iOS Shortcut provided by Stefan requires audio dictation in order to name the photo. Considering the fact that Siri has a funky support of Mandarin recognition, I create a new version with text input as file name. You can download the Shortcut here.  Overall, itâ€™s an awesome minimalist website and perfectly fits my â€œanti-instagramâ€ mindset. Very easy to deploy, strongly recommended to give it a try and maybe make some tweaks to it.\n","date":"2020-04-21","permalink":"//localhost:1313/post/2020-04-21-air-avo/","tags":["Food"],"title":"Air Avo is online!"},{"content":"tl;dr Implemented an alerting (kinda hard to call it \u0026ldquo;monitoring\u0026rdquo;) system for Django with R Server and a Telegram bot.\nA typical instance of its report is like this:\n Some errors occurred in the last 15 minutes on the server, a copy of log will be processed by sentRy. It will identify those new errors which have not been reported yet and save them to local storage. Meanwhile, the incremental part will be parsed to a telegram bot, sending error summaries and a recent 12-hour bar chart to a channel. At the same time, an updated copy of notifications (of course, errors) will be synced to Shinyapps.io and the Shiny app should have the latest info displayed.\n It was designed to fulfill a particular job and I guess it got things done to some extent. But recently, our dev team deployed a fully functional monitoring system called Sentry. I mean, what a coincidence. I had no idea about this and only named it after the sentry gun in Team Fortress 2.\nDependencies  tidyverse telegram.bot cronR shinyFiles aws.s3  How to use it (anyway)?  In Telegram, create a new bot under the permission of @BotFather. Follow the order and make sure you have a valid API Token. You can test the basic message functionality with bot_script.r. But it won\u0026rsquo;t be needed in the main script. You can also test run the actual process with log_process.r. If there are problems no more, click Addin in RStudio and select \u0026ldquo;Schedule R scripts on Linux/Unix\u0026rdquo;.  Related files  bot_script.r The script for testing Telegram bot creation. Will be dropped later. dashboard A shiny app displaying all the errors. error.log An error log copied from Django. global.r The script dedicated to setting up S3 connection. log_process.r The main script which needs to run periodically. notification.csv The formatted backup of errors captured by this monitoring script. It is de facto very similar to error.log. settings.csv A file to save some setting parameters including the latest reported error time. log_process.log The R console log for running log_process.r. Potentially useful when debugging.  Issues   Even the files published to shinyapps.io do not involve any unchecked files (when publishing), they will be verified any ways, thus leading to some filename and path related error returning. A better idea is to create a separate folder and zip it before uploading. Since I claimed all the paths absolute in my code due to the limitation of cronR, I have to scp another copy to the directory of shiny after writing to notification.\n  You can always refer to the \u0026ldquo;Log\u0026rdquo; tab in shiny app to debug. Really helpful.\n  Very hard to read a csv file without any column names in S3 bucket. The function aws.s3::s3read_using(FUN, ..., object, bucket, opts = NULL) is problematic, as FUN cannot insert any extra parameters. It is a shame that readr::read_delim cannot be used as well. At last a blog post on Medium saved my life.\n  Something weird happens when meta1 and meta2 extracted have different length. Specifically, meta1 with â€œERROR|WARNING|CRITICALâ€ has fewer than the number of rows. That is to say, some lines are not starting with â€œERRORâ€¦â€ Turns out my regex should start with a ^ otherwise things like \u0026quot; self._log(ERROR, msg, args, **kwargs)â€ could be matched as well. In short,\n  ^ ","date":"2020-02-14","permalink":"//localhost:1313/post/2020-02-14-sentry/","tags":["AWS","Telegram","R"],"title":"From sentRy to Sentry"},{"content":"ä¸¤ä¸ªå¥½ä¸œè¥¿ï¼š\nradian. å·ç§°æ˜¯ã€ŒA 21 century R consoleã€.\nå…¶å® 21 ä¸–çºªä¹Ÿéƒ½è¿‡äº† 20 å¹´äº†ï¼Œä¸‹ä¸ªæœˆ R 4.0 ä¹Ÿå°†èµ¶ç€ 20 å‘¨å¹´çš„æ—¥å­å‘å¸ƒã€‚ä½†è¿™éƒ½ä¸æ˜¯é‡ç‚¹ã€‚é‡ç‚¹æ˜¯æ¯æ¬¡ä¸ºäº†è¿è¡Œä¸€ä¸¤è¡ŒæŒ‡ä»¤è€Œæ‰“å¼€ã€Œç¬¨é‡ã€çš„ IDE ä»æ¥éƒ½ä¸æ˜¯æˆ‘æ‰€å–œæ¬¢çš„æ“ä½œè¡Œä¸ºã€‚ä¹‹å‰ç”¨è¿‡ console é‡Œçš„ Rï¼Œä½†æ€»è§‰å¾—å·®ç‚¹æ„æ€ã€‚å¦‚æœè¯´ç”¨ä¸€å¼ å›¾æ¥æ¦‚æ‹¬ radian çš„ä¼˜ç‚¹ï¼Œé‚£å°±æ˜¯è¿™æ ·çš„ï¼š\nå¦å¤–ä¸€ä¸ªæ˜¯è€æœ‹å‹ janitor package äº†ï¼Œæ›¾ç»åœ¨çœ‹ R for Data Science çš„æ—¶å€™ç”¨è¿‡ä¸€é˜µå­ï¼Œä½†æ˜¯ package è¿™ç§ä¸œè¥¿ä½ çœ‹åˆ°äº†ä¸ç”¨å°±åºŸäº†ï¼Œæˆ–è€…ç”¨è¿‡ä¸€æ¬¡ä¹‹åéš”ä¸€æ®µæ—¶é—´ä¸ç”¨ï¼Œä¹Ÿç­‰äºåºŸäº†ã€‚äºæ˜¯æŠ½ç©ºåœ¨ radian é‡ŒæŠŠå‡ ä¸ªä¸»è¦ functions è¿‡äº†ä¸€ä¸‹ï¼Œè§‰å¾—è¿˜æ˜¯åœ¨å¤„ç†ã€Œæ¯”è¾ƒè§„æ•´çš„ã€ç½‘ä¸Šä¸‹è½½çš„æ•°æ®ã€çš„æ—¶å€™æ¯”è¾ƒæ–¹ä¾¿ã€‚æ€»ç»“äº†ä¸€ä¸‹åŠŸç”¨ï¼š\n clean_names(). è½¬ä¸ºå°å†™+ä¸‹åˆ’çº¿çš„å˜é‡åæ ¼å¼ï¼Œèƒ½å¤Ÿå¤„ç†ç‰¹æ®Šç¬¦å·ã€‚ tably(). è¾“å…¥ vector è¾“å‡ºé¢‘ç‡å’Œé¢‘æ•°ç»Ÿè®¡ table.  å¦å¤–å¦‚æœå«æœ‰ NA åˆ™ä¸ä¼šè®¡å…¥ç»Ÿè®¡ã€‚ å¯ä»¥ç”¨ remove_empty(x, which = c(\u0026quot;rows\u0026quot;, â€œcolsâ€)) å»é™¤æ•´è¡Œå’Œï¼ˆæˆ–ï¼‰æ•´åˆ—ä¸º NAçš„æ•°æ®ã€‚ tably() åè·Ÿéšå¤šä¸ª vectors åˆ™è¾“å‡º crosstabulation ç»“æœã€‚   get_dupes(x). è¾“å…¥ data frame å’Œ å˜é‡å xï¼Œè¾“å‡ºé‡å¤çš„æ•°æ®ï¼ˆè¡Œï¼‰ã€‚ excel_numeric_to_data(). å°†æŸäº›è½½å…¥ Excel æ–‡ä»¶æ—¶è¢«è½¬åŒ–çš„æ—¥æœŸè¿˜åŸä¸º Date ç±»å‹ã€‚ ","date":"2020-01-31","permalink":"//localhost:1313/post/2020-01-31-radian-and-janitor/","tags":["R","Cleaning"],"title":"radian and janitor"},{"content":"Two weeks ago I sent my 2017 model of MacBook Pro to our local Genius Bar to replace the keyboard and the battery. During this one week span of absence, I moved to my iPad Pro to finish the Australian universities\u0026rsquo; course information scraping.\nSetup Initially I tried to deploy an server version of RStudio on my Ubuntu 19.04 server. But constant errors occured during the installation. After downgrading from 19.04 to 18.04. the installation went on smoothly.\nIn addtion, I installed zsh and oh-my-zsh as life-savers. (Guide)\nInstall R and RStudio server.\nStart RStudio Server will not permit logins by system users (those with user ids lower than 100).\nSo we need to create a \u0026ldquo;normal\u0026rdquo; user.\n$ sudo adduser rexarski // password==xingzilovexuanzi $ cat /etc/passwd($ grep '^rexarski' /etc/passwd  Unable to install R packages on Linux  Update and upgrade sudo apt updates and sudo apt upgrade sudo apt autoremove curl and sudo apt install curl no effect  ------------------------- ANTICONF ERROR --------------------------- Configuration failed because libcurl was not found. Try installing: * deb: libcurl4-openssl-dev (Debian, Ubuntu, etc) * rpm: libcurl-devel (Fedora, CentOS, RHEL) * csw: libcurl_dev (Solaris) If libcurl is already installed, check that 'pkg-config' is in your PATH and PKG_CONFIG_PATH contains a libcurl.pc file. If pkg-config is unavailable you can set INCLUDE_DIR and LIB_DIR manually via: R CMD INSTALL --configure-vars='INCLUDE_DIR=... LIB_DIR=...' -------------------------------------------------------------------- ERROR: configuration failed for package â€˜curlâ€™   Do these on root (who knows\u0026hellip;) https://askubuntu.com/questions/1057455/some-r-packages-wont-install  $ sudo apt-get install libcurl4-dev $ sudo apt-get install r-base-dev $ sudo apt-get install libssl-dev $ sudo apt-get install libxml2-dev   Then use a normal account (non-root) to install R packages.  install.packages(â€œtidyverseâ€, â€œrvestâ€, â€œpacmanâ€) # verify library(tidyverse)  Tidy problem  Takes forever to install tidyverse though. Also, pacman is not available for 3.4.4 Some packages are easy to install, but some takes forever like dplyr tidyr etc. Even loading an installed package dplyr is problematic:  Error: package or namespace load failed for â€˜dplyrâ€™ in library.dynam(lib, package, package.lib): shared object â€˜dplyr.soâ€™ not found In addition: Warning message: S3 methods â€˜[.fun_listâ€™, â€˜[.grouped_dfâ€™, â€˜all.equal.tbl_dfâ€™, â€˜anti_join.data.frameâ€™, â€˜anti_join.tbl_dfâ€™, â€˜arrange.data.frameâ€™, â€˜arrange.defaultâ€™, â€˜arrange.grouped_dfâ€™, â€˜arrange.tbl_dfâ€™, â€˜arrange_.data.frameâ€™, â€˜arrange_.tbl_dfâ€™, â€˜as.data.frame.grouped_dfâ€™, â€˜as.data.frame.rowwise_dfâ€™, â€˜as.data.frame.tbl_cubeâ€™, â€˜as.table.tbl_cubeâ€™, â€˜as.tbl.data.frameâ€™, â€˜as.tbl.tblâ€™, â€˜as.tbl_cube.arrayâ€™, â€˜as.tbl_cube.data.frameâ€™, â€˜as.tbl_cube.matrixâ€™, â€˜as.tbl_cube.tableâ€™, â€˜as_tibble.grouped_dfâ€™, â€˜as_tibble.tbl_cubeâ€™, â€˜auto_copy.tbl_cubeâ€™, â€˜auto_copy.tbl_dfâ€™, â€˜cbind.grouped_dfâ€™, â€˜collapse.data.frameâ€™, â€˜collect.data.frameâ€™, â€˜common_by.NULLâ€™, â€˜common_by.characterâ€™, â€˜common_by.defaultâ€™, â€˜common_by.listâ€™, â€˜compute.data.frameâ€™, â€˜copy_to.DBIConnectionâ€™, â€˜copy_to.src_localâ€™, â€˜default_missing.data.frameâ€™, â€˜default_missing.defaultâ€™, â€˜dim.tbl_cubeâ€™, â€˜distinct.data.f [... truncated]  Turns out, while installing readr i found there is not enough virtual memory. See this github issue: CRAN version of readr crashes the compiler on small-RAM Ubuntu 16.04 system. The Vultr server I was using is a $5 per month 1GB RAM little kicker, so this pretty much explains everything.\nThanks to this post. I call this â€œblack magicâ€. You can actually install a prebuilt binary so that you do not have to compile the package from the source.\n$ apt-get update $ apt-get install software-properties-common $ add-apt-repository -y â€œppa:marutter/rrutterâ€ $ add-apt-repository -y â€œppa:marutter/c2d4uâ€ $ apt-get update $ apt-get install r-cran-readr  Worked for me this time. But for packages like dplyr which were installed previously but could not be loaded on my end, I tried to remove.packages() them first, then installed the prebuilt version. The binary this time would not save the day because it is outdated and could not support the minimum requirement of rlang.\nBut anyways, this whole process at least suffices to power me through scraping a few bunch of universities courses already.\nI am officially missing my lil buddy MacBook.\n","date":"2019-11-25","permalink":"//localhost:1313/post/2019-11-25-r-on-ipad/","tags":["R","iOS","RStudio"],"title":"Writing R codes on an iPad Pro"},{"content":"å¦‚æœé¡ºåˆ©çš„è¯ï¼Œ30 åˆ†é’Ÿåº”è¯¥å¤Ÿã€‚å®é™…ä¸Šæˆ‘å¤§æ¦‚èŠ±äº† 2 ä¸ªå°æ—¶ï¼Œä¸»è¦å‘¨æ—‹äºå„å®¶é‚®ä»¶æœåŠ¡æä¾›å•†ä¹‹é—´ã€‚\n æœ€è¿‘å‘ç°å¾ˆå¤šäº§å“çš„å”®å/ç¤¾ç¾¤éƒ½åœ¨ç”¨ä¸€ä¸ªã€Œç¥ç§˜ã€çš„ bbs æ¡†æ¶ï¼Œä¸€ç›´ä¸çŸ¥é“å…¶å°Šå§“å¤§åã€‚æ¯”å¦‚Agenda, Airtable, è¿˜æœ‰æŸäº›ç¥ç§˜çš„å°è®ºå› ğŸ˜…. å¤šç•ªå¯»è®¿åœ¨ GitHub æ‰¾åˆ°äº†è¿™ä¸ªåå« Discourse çš„å¼€æºé¡¹ç›®â€”â€”GitHub - discourse/discourse: A platform for community discussion. Free, open, simple.\nå› ä¸ºé¡¹ç›®éœ€è¦ï¼Œæˆ‘æ‰“ç®—ç°åœ¨è‡ªå·±çš„ 1GB RAM å°æœåŠ¡å™¨ä¸Šå…ˆå°è¯•ä¸€ä¸‹ã€‚æ­£å¦‚ Discourse å›¢é˜Ÿåœ¨ä»–ä»¬çš„å¸®åŠ©æ–‡æ¡£ä¸­æ‰€è¨€ï¼Œæƒ³è¦æˆåŠŸéƒ¨ç½²ä¸€ä¸ªæœ€å°éœ€æ±‚çš„ç½‘ç«™éœ€è¦ä¸‰æ ·ä¸œè¥¿ï¼š\n ä¸€å°ã€Œåˆšåˆšå¥½å¤Ÿç”¨ã€çš„æœåŠ¡å™¨ã€‚ ä¸€ä¸ªã€Œä½ è§‰å¾— ğŸ‘Œ å°± ğŸ‘Œã€çš„ä¾¿å®œåŸŸåã€‚ ä¸€ä¸ªã€Œåœ¨å…è´¹å’Œæ”¶è´¹è¾¹ç¼˜å¾˜å¾Šã€çš„é‚®ä»¶å‘é€æœåŠ¡å•†ã€‚  æœåŠ¡å™¨æˆ‘ä½¿ç”¨çš„æ˜¯ Vultr çš„ 1GB RAM, 25GB SSD, æ”¶è´¹ä¸º $5/month. åŸŸååˆ™æ˜¯åœ¨ GoDaddy è´­ä¹°çš„ã€‚åæ§½ä¸€ä¸‹ GoDaddy çš„ DNS è®¾ç½®ï¼Œç›¸æ¯”äº AWS æ“ä½œç®€å•ä¸å°‘ï¼Œä½†å¼•å¯¼å…¶å®æœ‰ç‚¹æ¨¡ç³Šã€‚é‚®ä»¶å‘é€æœåŠ¡å•†æˆ‘æœ¬æƒ³å¯ç”¨å·²ç»ä½¿ç”¨äº†ä¸€æ®µæ—¶é—´çš„ Mandrillï¼Œä½†æ˜¯å‘ç°å®ƒåœ¨æˆ‘æŠŠ Discourse è®¾ç½®åœ¨ subdomain ä¸Šæ—¶æ€»æ˜¯è¡Œä¸é€šã€‚\nå®˜æ–¹çš„å¸®åŠ©æ–‡æ¡£åœ¨é‚®ä»¶çš„é—®é¢˜ä¸Šæ¨èäº†ä¸å°‘æœåŠ¡æä¾›å•†ï¼Œè€Œæ ¹æ®æˆ‘ä¸ªäººçš„æ‘¸ç´¢ï¼Œæœ€ç»ˆé€‰æ‹©çš„æ˜¯ SendGridã€‚Free tier åœ¨è°ƒè¯•é˜¶æ®µåŸºæœ¬å¤Ÿç”¨ï¼Œå½“ç„¶æ­£å¼ä¸Šçº¿è‚¯å®šæ˜¯ä¸è¡Œçš„ã€‚å…·ä½“çš„æµé‡é™åˆ¶)ä¸ºæ¯å¤© 100 å°å…è´¹ã€‚\næœåŠ¡å™¨çš„éƒ¨ç½²æŒ‰ç…§æ¨èèµ° Ubuntu LTS ç‰ˆæœ¬å°±å¯ä»¥äº† ï¼Œä¸ç”¨åšè¿‡å¤šçš„è°ƒè¯•ã€‚ä¹‹å ssh ç™»å½•ï¼Œå¹¶ git clone æ•´ä¸ª Discourse é¡¹ç›®åˆ° /var/discourse è·¯å¾„ä¸‹ã€‚\nç„¶åæ˜¯ä¸€ç³»åˆ—ç‚¹ç‚¹ç‚¹ç‚¹ä¹±æ“ä½œï¼š\n åœ¨ SendGrid ä¸­æ–°å»ºä¸€ä¸ª api key. åœ¨ SendGrid ä¸­ Settings \u0026ndash; Sender Authentication â€” Domain Authentication. éªŒè¯è‡ªå·±æ˜¯é‚®ä»¶å‘é€åŸŸåçš„çœŸå®æ‹¥æœ‰è€…ï¼Œè¿™ä¸€æ­¥å°±æŒ‰ç€æç¤ºèµ°ã€‚ ç„¶åè½¬åˆ° GoDaddy çš„ DNS Settings æ–°å¢ä¸‰æ¡ CNAME Record. è¿™é‡Œæœ‰ä¸€ä¸ªæš—å‘ï¼šSendGrid ä¼šæç¤ºä½ è¿™ä¸‰æ¡ records çš„ host æ˜¯ something.example.com ä½†æ˜¯ä½ ä¸èƒ½ç›´æ¥å¤åˆ¶ç²˜è´´åˆ° GoDaddy ä¸­ï¼Œè€Œåªèƒ½ç²˜è´´ something ä½œä¸º host. è€Œ record çš„ value åˆ™ç›´æ¥æ”¾å¿ƒç²˜è´´ã€‚ ç„¶ååœ¨ GoDaddy ä¸­åˆ›ä¸€ä¸ª A Record, host ä¸ºä½ æƒ³è®¾ç½®çš„ subdomain nameï¼Œæ¯”å¦‚ talk, discourse, forumï¼Œä»ç„¶ä¸è¦å¸¦ example.com. value åˆ™å¡«å…¥ä½ çš„æœåŠ¡å™¨ public ip åœ°å€ã€‚  å›åˆ°å·²ç» ssh è¿æ¥çš„æœåŠ¡å™¨ï¼š\ncd /var/discourse ./discourse-setup  è¿‡ç¨‹ä¸­ç¢°åˆ°ä¸€ä¸ªæŠ¥é”™ï¼Œè¿”å› No Public Key Error. å‚è€ƒ Stack Overflow çš„è§£ç­”ï¼Œå°‘ä»€ä¹ˆ KEYID å°±ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤æ·»åŠ å°±å¯ä»¥ äº†ï¼š\nsudo apt-key adv â€”keyserver keyserver.ubuntu.com â€”recv-keys KEYID  éƒ¨ç½²è¿‡ç¨‹ä¸­æœ‰å¤šæ¬¡éœ€è¦å¡«å†™çš„ä¿¡æ¯ï¼š\nHostname\t:\tsubdomain.example.com æ¨èä½¿ç”¨æ³¨å†Œæ³¨å†ŒåŸŸåçš„æŸä¸ª subdomain Email\t: me@anything.com éšä¾¿æŒ‘é€‰ä¸€ä¸ªè‡ªå·±åœ¨ç”¨çš„é‚®ç®±åœ°å€ä½œä¸º admin çš„æ³¨å†Œé‚®ç®± SMTP address\t:\tsmtp.sendgrid.net æ˜¯ email service provider çš„å…·ä½“è®¾ç½® SMTP port\t: 587 åŒä¸Š SMTP username\t:\tapikey\tåŒä¸Š SMTP password :\t*****\tç”Ÿæˆ api çš„æ—¶å€™ä¸€èˆ¬ä¼šç»™å‡º  å¤§çº¦ 3 åˆ° 5 åˆ†é’Ÿçš„å®‰è£…æ—¶é—´ï¼Œç„¶åå°±å¯ä»¥ç›´æ¥è®¿é—® subdomain.example.com æ¥æŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸã€‚ä¸€èˆ¬æ¥è¯´å®‰è£…çš„é—®é¢˜ä¸å¤§ï¼Œä¸‹ä¸€æ­¥éªŒè¯ admin é‚®ç®±æ‰æ¯”è¾ƒå®¹æ˜“å‡ºé”™ï¼Œå¦‚æœå¤Ÿå¹¸è¿ï¼Œä¹‹å‰æ²¡æœ‰è¸©åˆ°æš—å‘ä¸€è¹´è€Œå°±ï¼Œä½ çš„ me@anything.com é‚®ç®±é‡Œåˆ™ä¼šå¤šå‡ºä¸€å°é‚®ä»¶ï¼ˆæ³¨æ„æŸ¥ spam ğŸ“®ï¼‰ã€‚éªŒè¯å®Œæˆå°±ä¼šçœ‹åˆ°å¦‚å›¾æ‰€ç¤ºï¼š\nå¤§åŠŸå‘Šæˆã€‚\nå¦‚æœä¸­é—´ç¢°åˆ°äº†ä»€ä¹ˆé—®é¢˜ï¼Œç›´æ¥åœ¨æœåŠ¡å™¨ä¸Šï¼ˆ/var/discourse) é‡è·‘ä¸€é ./discourse-setup æŒ‡ä»¤å°±å¯ä»¥äº†ã€‚å›°éš¾ä¸»è¦è¿˜æ˜¯é›†ä¸­åœ¨ä¸‰é¡¹æœåŠ¡çš„é…ç½®ã€‚\nå¦å¤–æŒ‡å‡ºä¸€ç‚¹ï¼Œè™½ç„¶åœ¨æˆ‘å®‰è£…è¿‡ç¨‹ä¸­å¸®åŠ©ä¸å¤§ï¼Œä½† Discourse è‡ªå·±çš„å®˜æ–¹è®ºå›ä¹Ÿæœ‰ä¸å°‘å¯ä»¥å‚è€ƒçš„èµ„æºï¼Œæ¯” repo é‡Œ issue æœ‰ä»·å€¼å¤šäº†ã€‚åœ°å€æ˜¯ meta.discourse.org.\n","date":"2019-10-21","permalink":"//localhost:1313/post/2019-10-21-30-min-discourse/","tags":["Self-host","Linux"],"title":"30 åˆ†é’Ÿ Discourse è®ºå›æ­å»ºç¬”è®°"},{"content":"SCP scp is always our friend.\nscp -i /path/to/permission/file username@ec2.url:/path/to/remote/file /path/to/local/directory  Update on 2019-10-14. What if I need to copy files that need root access?\nssh -i /path/to/permission/file username@ec2.url \u0026quot;sudo cat /var/log/nginx/access.log\u0026quot; \u0026gt; ~/Downloads/access.log  Generally, if it is a log that could be printed out. Just print and save it to local.\n","date":"2019-09-30","permalink":"//localhost:1313/post/2019-09-30-ec2-download/","tags":["EC2","AWS"],"title":"How to download files from AWS EC2"},{"content":"A colleague had a coding interview for Huawei last Sunday. I heard the second question was quite â€œmathematicalâ€. Let me rephrase it here a little bit.\n A hero summoner in a MOBA game has an ability to manipulate three elements. By controlling the order of releasing these elements, he can cast different spells accordingly. For example, casting in the order of fire, water, lightening can be treated as a spell. But there are some limitations as well.\n  Consider fitting the elements of that spell in a cycle. Then turning the cycle clockwise or counterclockwise does not produce any new spells. Additionally, inverting the cycle will not generate new ones either. The question is, if n is the number of elements he is capable of mastering, m is the number of elements consisting a spell, then what is the value of the number of different spells modulo 1000000007?\n Typically, the mathematical term that describes the way of ordering elements, is called Circular Permutation.\nThe number of ways to arrange $n$ distinct objects along a fixed (i.e., cannot be picked up out of the plane and turned over) circle is\n$$ P_n=(n-1)!$$  The reason why it is the factorial of \\(n-1\\) instead of $n$ is all cycle rotation.\nIf we consider a stricter definition, there will be only three free permutations (i.e., inequivalent when flipping the circle is allowed).\n$$Pâ€™_n=\\frac{1}{2} (n-1)!, n\\geq 3$$ In our problem, the number would be\n$${n \\choose m} \\frac{1}{2} (m-1)!$$ Since \\(1\\leq m \\leq 10000, 1\\leq n \\leq 20000\\) , direct calculation of factorial is suicidal for a computer. The hack here should be using modulo arithmetic, namely, we only keep the mod of \\(10^9 \u0026#43; 7\\) in intermediate steps.\nfact \u0026lt;- function(n) { res \u0026lt;- 1 for (i in 1:n) { res \u0026lt;- (res * i) %% 1000000007 } return(res) }  Although factorial(203) will give us Inf as a result, fact(203 wonâ€™t. It will give us an exact answer of 572421883.\n","date":"2019-09-10","permalink":"//localhost:1313/post/2019-09-10-circular-permutation/","tags":["Maths","Graph Theory"],"title":"Circular Permutation"},{"content":"On Windows, we have a package called installr. Use function copy.packages.between.libraries(), then problem solved.\nOn macOS, unfortunately, we donâ€™t have that handy tool.\nBut we can still use the following to retrieve all current installed packagesâ€™ names:\nto_install \u0026lt;- as.vector(installed.packages()[,1]) install.packages(to_install)  A more concrete solution would be only updating those non-base-R packages:\ninstalled \u0026lt;- as.data.frame(installed.packages()) write.csv(installed, 'installed_previously.csv') installedPreviously \u0026lt;- read.csv('installed_previously.csv') baseR \u0026lt;- as.data.frame(installed.packages()) toInstall \u0026lt;- setdiff(installedPreviously, baseR) install.packages(toInstall[,1])  Still, I wish those old packages can be transferred to a new version of R painlessly.\n","date":"2019-07-24","permalink":"//localhost:1313/post/2019-07-24-the-lost-r-packages/","tags":["R"],"title":"The Lost R Packages (after updating R)"},{"content":"Inspired by Alex Onsager and his wacky web app Pokemon Fusion, I was thinking about improving his approaches myself. In his post Pokemon Fusion: Behind the Scenes, he explains how he makes the head-swapping. Basically this can be included in three steps:\n Manually prepare separate parts (a head and a body) of a PokÃ©mon. Determine the face size. Determine the main palette of a PokÃ©mon. Transplant the head to its new owner and unify the torso color.   Introducing the King in the Soil: Dugking!\n The resizing part is definitely a highlight. But some weird combinations could be easily noticed due to the \u0026ldquo;manual separation\u0026rdquo;, such as this Magikarp head on a Gyarados body:\nNevertheless, it is some brilliant work done by Alex.\nCrawler Alex mentions he plans only to support the first 151 PokÃ©mons for his site. Understandable, because it\u0026rsquo;s really hard to automate the head/torso separation. To be honest, I really wish there could be some magical facical recognition algorithm for PokÃ©mons.\nThe first thing is to scrape the sprites of PokÃ©mons. My target website is PokÃ©mon Database which has a pretty organized gallery of all 809 PokÃ©mons from 7 generations.\nScraping with rvest is not the hard part. The only thing tricky worth mentioning is the set the sleeping time between batches. How do I know this? Learn from trials.\nt0 \u0026lt;- Sys.time() # do things here t1 \u0026lt;- Sys.time() Sys.sleep(0.5*as.numeric(t1-t0))   Sleeping time is proportional to response time.\n Color picker What\u0026rsquo;s next?\n Load the images of PokÃ©mon A and PokÃ©mon B. Use K-means to find the main palette of the two. Simply swap the colors of two palettes. In fact, we are swapping two vectors of clustering assignments. Reconstruct the PokÃ©mon A with main palette of PokÃ©mon B. (I forgot to make the other way around, sorry.)  At this point, I\u0026rsquo;ve realized that facial recognition with data so far is not practical since all images are only of size 30x40 pixels. In other words, the data is too limited.\nAnyways, there are still something I want to point out during the four steps above.\n Although the scraped images are in PNG format with transparent background, once loaded with load.image(), the RGB of these transparent pixels will \u0026ldquo;turn to the darkside\u0026rdquo;. I am pretty sure in most of the cases, the top two clusterings are the background \u0026ldquo;Mr Black\u0026rdquo; and some darkish grey boundary. You can alwasy check their sizes to confirm.  When swapping the palettes between two PokÃ©mons, we can just ignore these two unnecessary colors.   Due to the randomness in K-means (km()), the generated clusterings differ slightly from time to time.  The number of clusterings \\(k\\) I pick here is 7. Just do not set it too large.     Are Red Gyarados extremely rare? Maybe.\nBut ladies and gentlemen, I present you the Elite Four of Blue Magikarps!\nAnd four mediocore red worms. Meh!\nAs I mentioned above, the head swapping process cannot be fulfilled with this scale of data. The least I can do is to implement the color swapping. You can find my codes in the repo below. Have fun playing with it!\nGitHub\n","date":"2019-04-17","permalink":"//localhost:1313/post/2019-04-17-pokemon-recoloring/","tags":["Scraping","Visualization","k-means"],"title":"PokÃ©mon Recoloring"},{"content":"Updated on 2020-08-29: I\u0026rsquo;ve migrated my blog from blogdown to hugo so this post might be outdated. See the original post for examples.\nUpdated on 2019-09-08: Today I realized the best approach to insert a centered image inside a html-supported markdown post is:\n\u0026lt;img class=\u0026quot;special-img-class\u0026quot; style=\u0026quot;width:100%\u0026quot; src=\u0026quot;url-to-image\u0026quot; /\u0026gt;   Two common misunderstanding for longtime blogdown lurkers (like me) or R markdown newbies:\n  Blogging with blogdown \\( \\equiv \\) writing reports with knitr. There is no need to \u0026ldquo;knit\u0026rdquo; every post you write, no matter which format (.md or .Rmd) you are using, since blogdown will handle them itself.\n  Deploying HTML inside a R markdown file can make the blog post more expressive than merely using Rmd syntax.\n  I didn\u0026rsquo;t realize the second one until when I was browsing Nan Xiao\u0026rsquo;s blog, who is the author of this theme I\u0026rsquo;m using. At one point, I was wondering why his post looks so different from mine. Out of curiosity, I digged a little into his raw files and came up with these following \u0026ldquo;hacks\u0026rdquo;.\nInsert an image with style This has nothing to do with direct usage of HTML, but still is different from traditional markdown rendering. R markdown will not render the name/caption of an image automatically, but it\u0026rsquo;s not the same case in blogdown here.\nI used to like inserting an image with placeholder name like img, so the R markdown codes would look like this:\n![img](path-to-this-image)  Somehow if this is the case, blogdown will render the image without any problem but leave you a weird image caption \u0026ldquo;img\u0026rdquo;.\nSolution: Either you ignore the img with a pair of blank square brackets,\n![](path-to-this-image)  or you figure out a nice caption for the image and do the rest like before. And you can even insert a hyperlink inside the caption. For example:\n![This is a [logo](https://www.r-project.org/Rlogo.png) of R project.](https://www.r-project.org/Rlogo.png)  Update on 2019-04-17: You might also want to center images in the post. This can be done with a pair of \u0026lt;center\u0026gt;\u0026lt;/center\u0026gt; tags.\nBut somehow the caption disappears. Try this:\n\u0026lt;center\u0026gt; ![This is a [logo](https://www.r-project.org/Rlogo.png) of R project.](https://www.r-project.org/Rlogo.png) \u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;Logo here!\u0026quot;\u0026lt;/p\u0026gt; \u0026lt;/center\u0026gt;  Embed a YouTube video Here\u0026rsquo;s something R markdown cannot do. We need to borrow the power from HTML..\nUpdate on 2019-04-17: According to this post \u0026ldquo;Use shortcodes to embed tweets, videos, etc. in your blogdown blog posts \u0026rdquo;, one line code can solve the problem:\nblogdown::shortcode(\u0026quot;youtube\u0026quot;, \u0026quot;jKp-WuK6iv8\u0026quot;)  Note that in the code blog, we need to flag the argument eval=TRUE.\nAdditionally, I guess one line of HTML as caption \u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;How could this happen?\u0026quot;\u0026lt;/p\u0026gt; will increase the \u0026ldquo;stylelishness\u0026rdquo; of an embedded media by all means.\n\u0026lt;p class=\u0026quot;caption\u0026quot;\u0026gt;\u0026quot;How could this happen?\u0026quot;\u0026lt;/p\u0026gt;  Pre-formatted table R markdown tables look nice most of the time. But sometimes we need more. With html tag \u0026lt;pre\u0026gt;\u0026lt;/pre\u0026gt;, one can easily display a pre-formatted table.\n\u0026lt;pre\u0026gt;\u0026lt;output\u0026gt; item1 item2 item3 item4 ============================= Gou Li Guo Jia ----------------------------- Qi Yin Huo Fu ============================= run_time: 59.0s total_time: 60.0s \u0026lt;/output\u0026gt;\u0026lt;/pre\u0026gt;  A real example of WYSIWYG.\nCode display One way to do so is, of course, to write in a code block. Alternatively, we can attach an external link like this:\n\u0026lt;script src=\u0026quot;https://gist.github.com/rexarski/92e120c3ffad70877716d99f6bfd66a3.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  And finally this \u0026hellip; I thought the shadow under some inserted images come from some extra HTML scripts. The fact is that it\u0026rsquo;s a native feature in macOS.\n Command + Shift + 4: Take a screen shot with area selection, traditionally. Command + Shift + 5: Bring out the screenshot tool bar. If a screenshot is taken to capture the selected window, it will generate some shadow below.  ","date":"2019-04-16","permalink":"//localhost:1313/post/2019-04-16-rmd-cheatsheet/","tags":["Rmarkdown","R"],"title":"What a Rmd Cheatsheet Doesn't Tell Me"},{"content":"OpenStreetMap\u0026rsquo;s OSRM (Open Source Routing Machine) provides not only routing service, but a feature that enables drawing multiple routes to form a time-based polygon range as well. This is also called an isochrone.\nAn isochrone, according to Wikipedia, is defined as \u0026ldquo;a line drawn on a map connecting points at which something occurs or arrives at the same time\u0026rdquo;. In other words, it is just like contour lines measuring travel time in urban design.\nTimothÃ©e Giraud has a repo osrm that connects OpenStreetMap\u0026rsquo;s service and R.\nIn the README file of this repo, there is also a vanilla example of how to draw osrmIsochrone.\nlibrary(osrm) library(sp) library(cartography) library(leaflet) # Get isochones with a SpatialPointsDataFrame, custom breaks iso \u0026lt;- osrmIsochrone(loc = c(149.1212457, -35.2766747), breaks = seq(from = 0,to = 30, by = 5)) # Map osm \u0026lt;- getTiles(x = iso, crop = FALSE, type = \u0026quot;osm\u0026quot;, zoom = 13) tilesLayer(x = osm) bks \u0026lt;- sort(c(unique(iso$min), max(iso$max))) cols \u0026lt;- paste0(carto.pal(\u0026quot;turquoise.pal\u0026quot;, n1 = length(bks)-1), 80) choroLayer(spdf = iso, var = \u0026quot;center\u0026quot;, breaks = bks, border = NA, col = cols, legend.pos = \u0026quot;topleft\u0026quot;,legend.frame = TRUE, legend.title.txt = \u0026quot;Isochrones\\n(min)\u0026quot;, add = TRUE) plot(apotheke.sp[10,], add=TRUE, col =\u0026quot;red\u0026quot;, pch = 20)  However, the processing time of this plot is not very satisfying. What about drawing the polygons with leaflet?\n# Map as leaflet iso@data$drive_times \u0026lt;- factor(paste(iso@data$min, \u0026quot;to\u0026quot;, iso@data$max, \u0026quot;min\u0026quot;)) factpal \u0026lt;- colorFactor(\u0026quot;RdYlBu\u0026quot;, iso@data$drive_times) leaflet() %\u0026gt;% setView(149.1212457, -35.2766747, zoom = 11) %\u0026gt;% addProviderTiles(\u0026quot;CartoDB.Positron\u0026quot;, group=\u0026quot;Greyscale\u0026quot;) %\u0026gt;% addMarkers(lng = 149.1212457, lat = -35.2766747, popup = \u0026quot;Starting Point\u0026quot;) %\u0026gt;% addPolygons(fill=TRUE, stroke=TRUE, color = \u0026quot;black\u0026quot;, fillColor = ~factpal(iso@data$drive_times), weight=0.5, fillOpacity=0.2, data = iso, popup = iso@data$drive_times, group = \u0026quot;Drive Time\u0026quot;) %\u0026gt;% addLegend(\u0026quot;bottomright\u0026quot;, pal = factpal, values = iso@data$drive_time, title = \u0026quot;Drive Time\u0026quot;)  An interactive map is not a bad idea. And can use proc.time() to track the processing time of both methods.\nptm \u0026lt;- proc.time() # plotting method proc.time() - ptm  Compare the processing of two plotting above. osrm takes around 50 seconds, while leaflet() only requires less than 7 seconds.\n user system elapsed 28.081 13.457 48.232 user system elapsed 5.692 0.173 6.532  ","date":"2019-04-01","permalink":"//localhost:1313/post/2019-04-01-travel-time-polygon/","tags":["R","Visualization"],"title":"Travel Time Polygon with osrm"},{"content":"Though Reddit\u0026rsquo;s comment sorting system has been in use for almost a decade, the fact is that I wasn\u0026rsquo;t so exposed to western internet community till 2013, it is still, a rather new thing to me.\nRandall, the author of xkcd, also my favourite internet comics, wrote a blog1 explaining what the algorithm is. Meanwhile, a detailed version is introduced by Evan Miller2.\nIn the latter, Evan gave two examples of two \u0026ldquo;wrong\u0026rdquo; ranking methods:\n Score = positive - negative; Score = positive / total.  To say they are \u0026ldquo;wrong\u0026rdquo;, does not mean that they are not possible to give a rough idea in all scenarios. But the reality is always complex, a rough idea is probably a synonym of \u0026ldquo;meaninglessness\u0026rdquo;. To be specific, the first case ignore the \u0026ldquo;ratio\u0026rdquo; part in the term of \u0026ldquo;highest rated\u0026rdquo;, that means a more controversial comment might exceed a quality post simply due to more people voting on it. The second case, however, ignores the scenario where the sample space is limited. For instance, we can hardly say that a 1 of 1 upvoted comment is better than that of 99 of 100 upvoted comment.\nHow Reddit deals with this problem is trying to reach a confident balance between positive proportion and small number of observations. Now suppose the following:\n Each voting event is independent. Each event can either be a positive or a negative. The total number of votes is \\( n \\) , the number of positive votes is \\( k \\) , \\( \\hat{p}=k/n \\) is the positive proportion.  Now the idea is to first find each \\( \\hat{p} \\) , then calculate the corresponding confidence intervals, and finally rank the items by their lower bounds of confidence intervals.\nThe perfect expression of these confidence intervals here is:\n$$ \\left(\\hat{p}+\\frac{z^2_{\\alpha/2}}{2n} \\pm z_{\\alpha/2}\\sqrt{[\\hat{p}(1-\\hat{p})+z^2_{\\alpha/2}/4n]/n}\\right)/(1+z^2_{\\alpha/2}/n) $$\nIn 1928, mathematician Edwin Bidwell Wilson3 developed this score interval above to estimate the successful (or positive, in our case) probability \\( \\hat{p} \\) .\nHere\u0026rsquo;s my R script to simulate a comment ranking situation possibly happening every day on Reddit.\nlibrary(dplyr) Wilson \u0026lt;- function(n, k, alpha=0.95) { phat \u0026lt;- k/n score \u0026lt;- qnorm(1-(1-alpha)/2) lbound \u0026lt;- (phat+score**2/(2*n)-score*sqrt((phat*(1-phat)+score**2/(4*n))/n))/(1+score**2/n) return(lbound) } seed(2019) x \u0026lt;- sample(1:500,200,replace=T) y \u0026lt;- sample(1:500,200,replace=T) votes \u0026lt;- data.frame(x,y) votes %\u0026gt;% rowwise() %\u0026gt;% mutate(pos=min(x,y),total=max(x,y)) %\u0026gt;% select(total, pos) %\u0026gt;% mutate(Wilson=Wilson(total,pos)) %\u0026gt;% arrange(desc(Wilson))  And the pseudo-results are:\n# A tibble: 200 x 3 total pos Wilson \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; 1 473 467 0.973 2 494 487 0.971 3 390 385 0.970 4 234 232 0.969 5 354 347 0.960 6 237 233 0.957 7 123 121 0.943 8 391 376 0.938 9 296 285 0.935 10 415 397 0.932 # ... with 190 more rows    redditâ€™s new comment sorting system. \u0026#x21a9;\u0026#xfe0e;\n How Not To Sort By Average Rating. \u0026#x21a9;\u0026#xfe0e;\n Binomial proportion confidence interval. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2019-03-02","permalink":"//localhost:1313/post/2019-03-02-reddit-ranking/","tags":["Algorithm","Maths"],"title":"What does Top-Rated Mean in Redditâ€™s Ranking System"},{"content":"ä¸»è¦æ˜¯ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯ condaï¼Œä¸€ä¸ªæ˜¯ MySQL.\nå‡†å¤‡  åœ¨Vulträ¸Šè´­ç½®ä¸€å°æœåŠ¡å™¨ï¼Œé€‰æ‹©$5/æœˆçš„é…ç½®å°±å·²ç»å¤Ÿäº†ï¼Œç³»ç»Ÿé€‰æ‹© CentOSã€‚å½“ç„¶äº†ï¼ŒUbuntu ä¹Ÿæ˜¯æå¥½çš„ã€‚ ä½¿ç”¨sshè¿æ¥æœåŠ¡å™¨ï¼Œip åœ°å€ä»¥åŠä¸´æ—¶å¯†ç éƒ½åœ¨ Vultr çš„ dashboard é‡Œå¯ä»¥æŸ¥é˜…ã€‚  conda  å®‰è£…Anaconda distributionã€‚å› ä¸ºæœåŠ¡å™¨ç«¯æ²¡æœ‰å›¾å½¢ç•Œé¢ï¼Œä¸€åˆ‡éƒ½åœ¨æˆ‘ä»¬çš„ terminal é‡Œå®Œæˆã€‚åœ¨æœåŠ¡å™¨ä¸­è¿è¡Œwget https://repo.continuum.io/archive/Anaconda3-2018.12-Linux-x86_64.sh ä¹‹å‰æœåŠ¡å™¨é€‰æ‹©çš„æ˜¯ 64 ä½ç³»ç»Ÿï¼Œè¿™é‡Œå°±ä» conda çš„é¡µé¢æ‰¾åˆ°å¯¹åº”çš„ 64 ä½ç³»ç»Ÿä¸‹è½½åœ°å€ã€‚ ç”±äº CentOS æ²¡æœ‰bzip2ï¼Œæ­¤æ—¶ç›´æ¥å®‰è£…åˆ™ä¼šæŠ¥é”™ï¼Œåº”å…ˆä½¿ç”¨yum install bzip2å®‰è£…ã€‚ æ¥ç€ä½¿ç”¨bash Anaconda3-*.*.*-Linux-x86_64.shå®‰è£… anaconda. ä¾ç…§æç¤ºï¼Œéƒ½å¾ˆç›´ç™½ã€‚ é»˜è®¤å®‰è£…è·¯å¾„ä¼šåœ¨~ä¹‹ä¸‹ï¼Œå®‰è£…å®ŒæˆåæŠŠå®‰è£…è·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡é‡Œã€‚ å¦‚æœæ­¤æ—¶æ²¡æœ‰æ–‡æœ¬ç¼–è¾‘å™¨çš„è¯ï¼Œå»ºè®®yum install vimï¼Œä¸´æ—¶æŠ±ä½›è„šå­¦ä¹ ä½¿ç”¨ä¸€ä¸‹ã€‚ ä½¿ç”¨vim ~/.bashrcæ‰“å¼€ç¯å¢ƒå˜é‡å‚¨å­˜æ–‡ä»¶.bashrc. Vim ä¸»è¦æœ‰ä¸¤ç§æ¨¡å¼ï¼Œä¸€ç§æ˜¯æ’å…¥æ¨¡å¼ (insert mode)ï¼Œä¸€ç§æ˜¯æŒ‡ä»¤æ¨¡å¼ (command mode)ã€‚å½“å‰æˆ‘ä»¬å¤„äºæŒ‡ä»¤æ¨¡å¼ä¸‹ï¼ŒæŒ‰ié”®åˆ‡å…¥åˆ° insert modeï¼Œç„¶ååœ¨æ–‡ä»¶æœ«ç«¯å½•å…¥ä»£ç export PATH=â€œ$HOME/anaconda3/bin:$PATHâ€. æŒ‰Escé”®é€€å‡ºåˆ° command modeï¼Œå½•å…¥:wqä¿å­˜å¹¶é€€å‡ºï¼Œæ­¤æ—¶å…‰æ ‡ä¼šå‡ºç°åœ¨ vim çš„æœ€åä¸€è¡Œã€‚æ¨å‡ºä¹‹åï¼Œè®°å¾—source ~/.bashrcä¸€ä¸‹ã€‚ç„¶åæˆ‘ä»¬å°±å¯ä»¥ç”¨ conda çš„æŒ‡ä»¤æ¥æ£€æŸ¥ conda æ˜¯å¦æˆåŠŸå®‰è£…ï¼Œæˆ–è€…å®‰è£…å…¶ä»–çš„ packageã€‚ æ­¤æ—¶ conda çš„å®‰è£…å‘Šä¸€æ®µè½ï¼Œæ¥ä¸‹æ¥æ˜¯ MySQL æ•°æ®åº“çš„éƒ¨ç½²ã€‚  MySQL  åœ¨MySQL å®˜ç½‘å¯»æ‰¾ yum æºï¼Œç±»ä¼¼çš„æ–¹æ³•ï¼Œæ‰¾åˆ°é“¾æ¥å°±è¡Œã€‚è¿™é‡Œå› ä¸ºæˆ‘ä»¬ç”¨çš„æ˜¯ CentOSï¼Œä½¿ç”¨ Red Hat Enterprise Linux 7 ç‰ˆæœ¬ã€‚yum localinstall https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm yum install mysql-community-server -y, å¹¶ç”¨mysqld --versionæŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸã€‚ é…ç½® MySQL ä½¿å…¶è‡ªåŠ¨å¯åŠ¨systemctl enable mysqld.service. å¯åŠ¨systemctl start mysqld.service. å¯¹äº 5.7 ä»¥ä¸Šç‰ˆæœ¬çš„ MySQLï¼Œè¿˜éœ€è¦ä»¥ä¸‹çš„æ–¹æ³•ä¿®æ”¹é»˜è®¤ root ç”¨æˆ·çš„ç™»å½•å¯†ç ã€‚ç¬¬ä¸€æ­¥æ˜¯å…ˆæ‰¾åˆ°ä¸´æ—¶å¯†ç ï¼šsudo grep 'temporary password' /var/log/mysqld.log mysql -u root -påæ¥ temporary password è¿›å…¥ MySQL. MySQL ç°åœ¨çš„å¯†ç è¦æ±‚è§„åˆ™æ¯”è¾ƒå¤æ‚ï¼Œå¦‚æœè‡ªå·±æƒ³è®¾ç½®ç®€å•å¯†ç ï¼Œåˆ™éœ€è¦SET GLOBAL validate_password.policy=LOW;. æœ€åä¿®æ”¹å¯†ç ALTER USER 'root'@'localhost' IDENTIFIED BY 'mynewpassword'; \\qé€€å‡º MySQL, exité€€å‡º ssh è¿æ¥ã€‚  åˆ°æ­¤ä¸ºæ­¢å°±åŸºæœ¬èƒ½ç”¨äº†ï¼Œç„¶åå°±æ˜¯ develop, play, and enjoy.\n","date":"2019-01-12","permalink":"//localhost:1313/post/2019-01-02-deploy-a-server-for-data-science/","tags":["conda","MySQL","Linux"],"title":"å¦‚ä½•éƒ¨ç½²ä¸€å°æ•°æ®ç§‘å­¦ä¸“ç”¨çš„ç®€æ˜“æœåŠ¡å™¨"},{"content":"I\u0026rsquo;ve always been wondering if Apple could utilize its collected health data in a more elegant way. The Activity app is decent, especially the achievements inside are definitely highlights in an iPhone x Apple Watch dynamic duo. On the other hand, the Health app serves nothing but a centralized information hub for various types data collected by either the sensor in your iPhone or your Apple Watch.\nWell, that\u0026rsquo;s basically my perspective and I don\u0026rsquo;t have any constructive suggestions yet. But one thing that always interests me is that if I can play some magic around those hidden data. The answer is an absolute yes. The data export from iPhone is quite easy, and kudos to Apple. The problem is, the data comes in JSON format. Ryan Praskievicz provided a Python script to convert those extracted JSON to csv, but also he attached a link to an online converter here.\nI initially had the very first model of Apple Watch, aka the \u0026ldquo;Apple Watch before Series 1\u0026rdquo;, aka \u0026ldquo;Series 0\u0026rdquo;, which should be considered as a crappy smart watch even back in a scenario 4 years ago. The battery life could hardly power through one day, and launching a third-party app could literally take half a minute. So the only two functions I used were time keeping and activaity monitoring. Unfortunately, the watch encountered a serious this May and I still haven\u0026rsquo;t figured out what happened to it. But in short, the watch stopped syncing with my iPhone and did not track any movements either. I had to perform a hard reset and thus lost all data prior to that day. So far, the data sitting on my MacBook is my Apple Watch health data collected from early this May to October.\nThe size of the data is around 90 mb and it wouldn\u0026rsquo;t take too long to convert them to csv. But this really depends. I mean if you have a load of several years\u0026rsquo; data, I strongly suggest you make yourself a cup of tea during this time. I haven\u0026rsquo;t checked the data structure inside at that moment, but a vague goal is to mimic the \u0026ldquo;major three\u0026rdquo; on Apple Watch: the activities, the exercise time and the standing hours. Surprisingly, the exported data has no standing hours, so that I have to use daily steps as a substitute.\nlibrary(tidyverse) library(lubridate) library(gridExtra) library(RColorBrewer) read_one_field \u0026lt;- function(filename) { file \u0026lt;- read_delim(paste0(\u0026quot;./\u0026quot;, filename), delim=\u0026quot;;\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;)) # glimpse(file) if (str_detect(file$type[1], regex(\u0026quot;ActiveEnergyBurned\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: kcal mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(active = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(active = sum(active)) } else if (str_detect(file$type[1], regex(\u0026quot;AppleExerciseTime\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: min mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(exercise = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(exercise = sum(exercise)) } else if (str_detect(file$type[1], regex(\u0026quot;StepCount\u0026quot;, ignore_case = TRUE))) { core_data \u0026lt;- file %\u0026gt;% select(creationDate, value) %\u0026gt;% # unit: count mutate(creationDate = as_datetime(creationDate)) %\u0026gt;% rename(step = value) %\u0026gt;% group_by(day = date(creationDate)) %\u0026gt;% summarise(step = sum(step)) } } active \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierActiveEnergyBurned.csv\u0026quot;) exercise \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierAppleExerciseTime.csv\u0026quot;) step \u0026lt;- read_one_field(\u0026quot;/data/apple_health_export/HKQuantityTypeIdentifierStepCount.csv\u0026quot;)  The csv files are pretty neat overall, only a few NAs to deal with. Afterwards, I notice that each entry is responsible for an time elapse ranging from several seconds to minutes. Hence aggregation is unavoidable. The most natural way to do so is to group them by days.\n# standard: active = 400, exercise = 30, step = 10000 full_data \u0026lt;- active %\u0026gt;% full_join(exercise) %\u0026gt;% full_join(step) %\u0026gt;% replace_na(list(active = 0, exercise = 0, step = 0)) %\u0026gt;% mutate(active = active / 400, exercise = exercise / 30, step = step / 10000) %\u0026gt;% gather(key = type, value = value, -day)  Soon I come up with a line plot using a cutomized theme, which seems pretty similar to Apple\u0026rsquo;s Activity color palette. The theme is based on a pure black ggplot2 theme on GitHub.\ntheme_black = function(base_size = 12, base_family = \u0026quot;\u0026quot;) { theme_grey(base_size = base_size, base_family = base_family) %+replace% theme( # Specify axis options axis.line = element_blank(), axis.text.x = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;, lineheight = 0.9), axis.text.y = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;, lineheight = 0.9), axis.ticks = element_line(color = \u0026quot;white\u0026quot;, size = 0.2), axis.title.x = element_text(size = base_size, color = \u0026quot;white\u0026quot;, margin = margin(0, 10, 0, 0)), axis.title.y = element_text(size = base_size, color = \u0026quot;white\u0026quot;, angle = 90, margin = margin(0, 10, 0, 0)), axis.ticks.length = unit(0.3, \u0026quot;lines\u0026quot;), # Specify legend options legend.background = element_rect(color = NA, fill = \u0026quot;black\u0026quot;), legend.key = element_rect(color = \u0026quot;white\u0026quot;, fill = \u0026quot;black\u0026quot;), legend.key.size = unit(1.2, \u0026quot;lines\u0026quot;), legend.key.height = NULL, legend.key.width = NULL, legend.text = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;), legend.title = element_text(size = base_size*0.8, face = \u0026quot;bold\u0026quot;, hjust = 0, color = \u0026quot;white\u0026quot;), legend.position = \u0026quot;right\u0026quot;, legend.text.align = NULL, legend.title.align = NULL, legend.direction = \u0026quot;vertical\u0026quot;, legend.box = NULL, # Specify panel options panel.background = element_rect(fill = \u0026quot;black\u0026quot;, color = NA), panel.border = element_rect(fill = NA, color = \u0026quot;white\u0026quot;), panel.grid.major = element_line(color = \u0026quot;grey35\u0026quot;), panel.grid.minor = element_line(color = \u0026quot;grey20\u0026quot;), panel.margin = unit(0.5, \u0026quot;lines\u0026quot;), # Specify facetting options strip.background = element_rect(fill = \u0026quot;grey30\u0026quot;, color = \u0026quot;grey10\u0026quot;), strip.text.x = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;), strip.text.y = element_text(size = base_size*0.8, color = \u0026quot;white\u0026quot;,angle = -90), # Specify plot options plot.background = element_rect(color = \u0026quot;black\u0026quot;, fill = \u0026quot;black\u0026quot;), plot.title = element_text(size = base_size*1.2, color = \u0026quot;white\u0026quot;), plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), \u0026quot;cm\u0026quot;) ) }  ggplot(full_data, aes(x=day, y=value)) + geom_line(aes(color=type),size=1) + scale_color_manual(labels = c(\u0026quot;Active Energy\u0026quot;, \u0026quot;Exercise Time\u0026quot;, \u0026quot;Steps\u0026quot;), values=c(\u0026quot;#FF0054\u0026quot;, \u0026quot;#8CFF00\u0026quot;, \u0026quot;#00FFCA\u0026quot;)) + theme_black() + facet_grid(type ~ .) + labs(title=\u0026quot;Apple Watch Health Data\u0026quot;, subtitle=\u0026quot;what to write here\u0026quot;, caption=\u0026quot;Source: my retired Apple Watch (1st gen)\u0026quot;, y=\u0026quot;Complete 100%\u0026quot;, color=NULL)  But I\u0026rsquo;m not satisfied with theme somehow, so I decide to experiment a little bit on a FiveThirtyEight-style theme. It is a more furnished theme, but does not match the Activity app palette. Well, I just cannot reject the magic of minimalism.\nfte_theme \u0026lt;- function() { # Generate the colors for the chart procedurally with RColorBrewer palette \u0026lt;- brewer.pal(\u0026quot;Greys\u0026quot;, n=9) color.background = palette[2] color.grid.major = palette[3] color.axis.text = palette[6] color.axis.title = palette[7] color.title = palette[9] # Begin construction of chart theme_bw(base_size=9) + # Set the entire chart region to a light gray color theme(panel.background=element_rect(fill=color.background, color=color.background)) + theme(plot.background=element_rect(fill=color.background, color=color.background)) + theme(panel.border=element_rect(color=color.background)) + # Format the grid theme(panel.grid.major=element_line(color=color.grid.major,size=.25)) + theme(panel.grid.minor=element_blank()) + theme(axis.ticks=element_blank()) + # Format the legend, but hide by default theme(legend.position=\u0026quot;none\u0026quot;) + theme(legend.background = element_rect(fill=color.background)) + theme(legend.text = element_text(size=7,color=color.axis.title)) + # Set title and axis labels, and format these and tick marks theme(plot.title=element_text(color=color.title, size=10, vjust=1.25)) + theme(axis.text.x=element_text(size=7,color=color.axis.text)) + theme(axis.text.y=element_text(size=7,color=color.axis.text)) + theme(axis.title.x=element_text(size=8,color=color.axis.title, vjust=0)) + theme(axis.title.y=element_text(size=8,color=color.axis.title, vjust=1.25)) + # Plot margins theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), \u0026quot;cm\u0026quot;)) }  ggplot(full_data, aes(x=day, y=value)) + geom_line(aes(color=type),size=1) + fte_theme() + facet_grid(type ~ .) + labs(title=\u0026quot;Apple Watch Health Data\u0026quot;, subtitle=\u0026quot;what to write here\u0026quot;, caption=\u0026quot;Source: my retired Apple Watch (1st gen)\u0026quot;, y=\u0026quot;Complete 100%\u0026quot;, color=NULL)  But even now I\u0026rsquo;m not so sure about what I can really do with these data, except for probably plotting them with lines and dots. Meanwhile, I\u0026rsquo;m not okay with the variable on y-axis as well. I tried to combine them in one plot at the very beginning, but it turned out that the three types of data were not on the same scale. So I made them the percentages of the completeness of daily goals by setting activity to 400, exercise time to 30 minutes and steps to 10,000. But still, the y-axis should better be \u0026ldquo;the ratio to the goal\u0026rdquo; instead of a bunch of misleading integers (1%? 2%?)\nWell, at least I can conclude that all these match the title which says \u0026ldquo;a sneak peek\u0026rdquo;, not a thorough digging.\n","date":"2018-11-26","permalink":"//localhost:1313/post/2018-11-26-apple-watch-health-data/","tags":["iOS","ggplot2","Visualization"],"title":"A Sneak Peek at Apple Watch Health Data"},{"content":"So today marks the release date of the latest Hearthstone expansion Dr. Boom\u0026rsquo;s Project The Boomsday Project. As usual, I\u0026rsquo;ve done some pack opening with my pal. But this time, I\u0026rsquo;ve also taken down some data and made a simple scatterplot (maybe?) just for fun.\nSome results  I was probably too sleepy at 3 am in the morning, lost 2 packs\u0026rsquo; data. The total pack number should be 120 instead of 118. Including the guaranteed one in the first 10 packs, I discovered 9 legendary cards in total. But unfortunately, none of them is golden. The first 8 legendary cards were opened in the first 60 packs. Pretty imbalanced! The most common card combination is, of course, 4 commons + 1 rare (Let\u0026rsquo;s call it \u0026ldquo;4c1r\u0026rdquo;). Out of all my 118 packs, 66 of them are of this type, which take about 56% of total pack opening. (Blizzard you are too greedy!) The longest streak of 4c1r is 6.  This post might not have any actual meaning. But I\u0026rsquo;m generally happy with the results since at least I\u0026rsquo;ve got the leading role of this expansion: Dr. Boom.\n","date":"2018-08-08","permalink":"//localhost:1313/post/2018-08-08-hearthstone/","tags":["R","Visualization","ggplot2"],"title":"Visualizing Hearthstone Pack Opening with Animation"},{"content":"The post here demonstrates an example of Hacker News scraper with rvest library.\nBut a tiny problem emerges as sometimes YC-funded startup post job ads on the front page so that the scraper would find different information vector with different lengths. Specifically, the it would not return any score value for that ad. One possible remedy is to log in as a real user, then do the scraping. In R, it is implemented as:\nlibrary(rvest) login \u0026lt;- 'https://news.ycombinator.com/login' session \u0026lt;- html_session(login) form \u0026lt;- html_form(session)[[1]] filled_form \u0026lt;- set_values(form, acct='MyAccountName', pw='MyPassword') submit_form(session, filled_form) content \u0026lt;- jump_to(session, 'https://news.ycombinator.com/') title \u0026lt;- content %\u0026gt;% html_nodes('a.storylink') %\u0026gt;% html_text() link_domain \u0026lt;- content %\u0026gt;% html_nodes('span.sitestr') %\u0026gt;% html_text() score \u0026lt;- content %\u0026gt;% html_nodes('span.score') %\u0026gt;% html_text() age \u0026lt;- content %\u0026gt;% html_nodes('span.age') %\u0026gt;% html_text() df \u0026lt;- data.frame(title = title, link_domain = link_domain, score = score, age = age)  The saved data frame would contain all the 30 links on the front page of Hacker News.\n","date":"2018-06-26","permalink":"//localhost:1313/post/2018-06-26-basic-web-scraping-with-rvest/","tags":["Scraping","R"],"title":"Basic Web Scraping with rvest"},{"content":"The design was inspired by a blog on Fronkonstin: Experiments in R. I modified the code a little bit, so that the output is generated by Peter de Jong Attractors instead of Clifford Attractors.\nThe definitive expressions of Peter de Jong Attractors are shown below:\n$$ \\begin{aligned} x_{n+1} \u0026amp;= \\sin(a \\cdot y_n) - \\cos(b \\cdot x_n) \\cr y_{n+1} \u0026amp;= \\sin(c \\cdot x_n) - \\cos(d \\cdot y_n) \\end{aligned} $$\nCodes:\n The output looks really pretty good, like a basket. You can visit this link for more examples and detailed parameters.\n","date":"2018-01-08","permalink":"//localhost:1313/post/2018-01-08-de-jong-attractors/","tags":["Maths","Generative"],"title":"Drawing Trajectory of Peter de Jong Attractors with ggplot2"}]